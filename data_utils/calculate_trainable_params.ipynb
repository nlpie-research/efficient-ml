{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (LoraConfig,IA3Config, PeftType, PrefixTuningConfig,\n",
    "                  PromptEncoderConfig, PromptTuningConfig, TaskType,\n",
    "                  get_peft_config, get_peft_model, get_peft_model_state_dict,\n",
    "                  prepare_model_for_int8_training,\n",
    "                  prepare_model_for_kbit_training, set_peft_model_state_dict)\n",
    "from scipy.special import softmax\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForTokenClassification, AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          LlamaForSequenceClassification, LlamaTokenizer,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          get_linear_schedule_with_warmup, set_seed)\n",
    "from model_utils import count_trainable_parameters, get_model_size, get_full_model_size\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "# import sys and append path\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from peft_trainer import create_peft_config\n",
    "from loguru import logger as loguru_logger\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arg_dict = {\n",
    "  \"log_save_dir\": \"/mnt/sdc/niallt/saved_models/peft_training/logs\",\n",
    "  \"task_type\": \"SEQ_CLS\",\n",
    "  \"peft_method\": \"LORA\",\n",
    "  \"lora_rank\": 8, \n",
    "  \"lora_alpha\": 16,\n",
    "  \"lora_dropout\": 0.1,\n",
    "  \"learning_rate\": 3e-4,\n",
    "  \"num_virtual_tokens\": 10 \n",
    "}\n",
    "\n",
    "# convert above args to a namespace object\n",
    "\n",
    "args = Namespace(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_type_mappings = {\"roberta\": \"roberta-base\",\n",
    "               \"mobile\": \"nlpie/bio-mobilebert\",\n",
    "               \"distil\": \"nlpie/distil-biobert\",\n",
    "               \"tiny\": \"nlpie/tiny-biobert\",\n",
    "               \"llama-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "               \"bert\": \"dmis-lab/biobert-v1.1\",\n",
    "               }\n",
    "\n",
    "peft_types = [\"PROMPT_TUNING\",\"LORA\", \"PREFIX_TUNING\", \"P_TUNING\"]\n",
    "\n",
    "def get_number_of_trainable_params(model_type_mappings:dict,\n",
    "                                   peft_types:list,\n",
    "                                   task_type:str = \"SEQ_CLS\",\n",
    "                                   num_labels:int = 2):\n",
    "\n",
    "    # set up empty dicts to full for dfs\n",
    "    model_peft_dict = {}\n",
    "    \n",
    "    for model_type in model_type_mappings.keys():\n",
    "        \n",
    "        model_dict = {}\n",
    "        model_name_or_path = model_type_mappings[model_type]\n",
    "        model_args = dict(pretrained_model_name_or_path=model_name_or_path, \n",
    "                          num_labels=num_labels, \n",
    "                          output_hidden_states=False, \n",
    "                          trust_remote_code=True)\n",
    "\n",
    "            \n",
    "        if task_type == \"SEQ_CLS\":\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(**model_args)\n",
    "        elif task_type == \"TOKEN_CLS\":\n",
    "            model = AutoModelForTokenClassification.from_pretrained(**model_args)\n",
    "        \n",
    "        # falcon model seems to use model config to define pad token and the remote code panicks if you don't set it\n",
    "        if \"falcon\" in model_name_or_path:\n",
    "            model.config.use_cache = False            \n",
    "\n",
    "        # count total trainable params before peft\n",
    "        total_trainable_params = count_trainable_parameters(model)\n",
    "        \n",
    "        # get model size and full model size too\n",
    "        model_size_MB, model_size_GB = get_model_size(model)\n",
    "        full_model_size_MB, full_model_size_GB = get_full_model_size(model)\n",
    "        \n",
    "        for peft_method in tqdm(peft_types, desc=f\"model type: {model_type}\"):\n",
    "            \n",
    "            \n",
    "            # set up some PEFT params\n",
    "            peft_config, lr = create_peft_config(args, peft_method, model_name_or_path,task_type)\n",
    "            model = get_peft_model(model, peft_config)\n",
    "            print(f\"peft config is: {peft_config}\")\n",
    "            # print(model)\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            # lets also confirm this directly and save to args\n",
    "            n_trainable_params = count_trainable_parameters(model)\n",
    "            # proportion of total trainable params\n",
    "            n_trainable_params_perc = (n_trainable_params / total_trainable_params) * 100\n",
    "            \n",
    "            # get size of peft adapter only\n",
    "            peft_model_size_MB, peft_model_size_GB = get_model_size(model)\n",
    "            peft_full_model_size_MB, peft_full_model_size_GB = get_full_model_size(model)\n",
    "            \n",
    "            # store the model name, peft method and number of trainable params\n",
    "            model_dict[peft_method] = {\"n_trainable_params\": n_trainable_params,\n",
    "                                 \"total_trainable_params\": total_trainable_params,\n",
    "                                 \"n_trainable_params_perc\": n_trainable_params_perc,\n",
    "                                 \"model_size_MB\": model_size_MB,\n",
    "                                 \"model_size_GB\": model_size_GB,\n",
    "                                 \"full_model_size_MB\": full_model_size_MB,\n",
    "                                 \"full_model_size_GB\": full_model_size_GB,\n",
    "                                 \"peft_model_size_MB\": peft_model_size_MB,\n",
    "                                 \"peft_model_size_GB\": peft_model_size_GB,\n",
    "                                 \"peft_full_model_size_MB\": peft_full_model_size_MB,\n",
    "                                 \"peft_full_model_size_GB\": peft_full_model_size_GB,}\n",
    "            \n",
    "        model_peft_dict[model_type] = model_dict\n",
    "\n",
    "    return model_peft_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 475.49121856689453\n",
      "Total size (MB): 951.0656957626343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: roberta:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:27:50.842 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='roberta-base', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 599,810 || all params: 125,246,980 || trainable%: 0.4789017667332178\n",
      "Model size (MB): 2.2880935668945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: roberta:  25%|██▌       | 1/4 [00:00<00:01,  1.98it/s]2023-11-10 11:27:51.349 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 480.15754413604736\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'value', 'query'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 887,042 || all params: 125,541,892 || trainable%: 0.7065705206991783\n",
      "Model size (MB): 3.3837966918945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: roberta:  50%|█████     | 2/4 [00:01<00:01,  1.99it/s]2023-11-10 11:27:51.849 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 482.42277240753174\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 776,450 || all params: 125,718,532 || trainable%: 0.6176098206428309\n",
      "Model size (MB): 2.9619216918945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: roberta:  75%|███████▌  | 3/4 [00:01<00:00,  2.03it/s]2023-11-10 11:27:52.330 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 482.682110786438\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 813,826 || all params: 125,755,908 || trainable%: 0.6471473292531115\n",
      "Model size (MB): 3.1044998168945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: roberta: 100%|██████████| 4/4 [00:01<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 482.9772653579712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 93.77637481689453\n",
      "Total size (MB): 187.98572540283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:27:54.439 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/bio-mobilebert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=128, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 2,306 || all params: 24,585,220 || trainable%: 0.00937961913702623\n",
      "Model size (MB): 0.00879669189453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  25%|██▌       | 1/4 [00:00<00:00,  7.12it/s]2023-11-10 11:27:54.581 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n",
      "2023-11-10 11:27:54.582 | INFO     | peft_trainer:create_peft_config:673 - Using mobile config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 94.25250720977783\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'key', 'value', 'query'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 222,210 || all params: 24,806,404 || trainable%: 0.895776751841984\n",
      "Model size (MB): 0.8476638793945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  50%|█████     | 2/4 [00:00<00:00,  5.25it/s]2023-11-10 11:27:54.806 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 96.08641529083252\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_hidden_size=512, prefix_projection=False)\n",
      "trainable params: 246,786 || all params: 25,050,884 || trainable%: 0.9851388877135034\n",
      "Model size (MB): 0.9414138793945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  75%|███████▌  | 3/4 [00:00<00:00,  5.53it/s]2023-11-10 11:27:54.975 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 97.14476490020752\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 154,370 || all params: 24,958,468 || trainable%: 0.6185075141631289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile: 100%|██████████| 4/4 [00:00<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 0.5888748168945312\n",
      "Total size (MB): 96.47434902191162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/distil-biobert and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 250.94824981689453\n",
      "Total size (MB): 501.94249629974365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:27:56.109 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/distil-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 9,218 || all params: 65,793,796 || trainable%: 0.014010439525331536\n",
      "Model size (MB): 0.03516387939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  25%|██▌       | 1/4 [00:00<00:01,  2.88it/s]2023-11-10 11:27:56.458 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 251.0684061050415\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'value', 'query'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 148,994 || all params: 65,941,252 || trainable%: 0.22594960738689038\n",
      "Model size (MB): 0.5683670043945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  50%|█████     | 2/4 [00:00<00:00,  3.03it/s]2023-11-10 11:27:56.775 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 252.18628025054932\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 93,698 || all params: 66,025,732 || trainable%: 0.14191133844604706\n",
      "Model size (MB): 0.35742950439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  75%|███████▌  | 3/4 [00:00<00:00,  3.15it/s]2023-11-10 11:27:57.078 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 252.30157566070557\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 223,234 || all params: 66,155,268 || trainable%: 0.3374394915912063\n",
      "Model size (MB): 0.8515701293945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 253.29643726348877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 52.928016662597656\n",
      "Total size (MB): 105.88920783996582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: tiny:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:27:57.733 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n",
      "2023-11-10 11:27:57.780 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n",
      "model type: tiny:  50%|█████     | 2/4 [00:00<00:00, 19.30it/s]2023-11-10 11:27:57.838 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n",
      "2023-11-10 11:27:57.889 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/tiny-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 3,746 || all params: 13,878,508 || trainable%: 0.026991374000721116\n",
      "Model size (MB): 0.01428985595703125\n",
      "Total size (MB): 52.99280643463135\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'value', 'query'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 40,562 || all params: 13,918,444 || trainable%: 0.29142625425658214\n",
      "Model size (MB): 0.15473175048828125\n",
      "Total size (MB): 53.30037784576416\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_hidden_size=312, prefix_projection=False)\n",
      "trainable params: 25,586 || all params: 13,940,284 || trainable%: 0.1835400197011768\n",
      "Model size (MB): 0.09760284423828125\n",
      "Total size (MB): 53.32936954498291\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 100,570 || all params: 14,015,268 || trainable%: 0.7175745765261142\n",
      "Model size (MB): 0.38364410400390625\n",
      "Total size (MB): 53.90683650970459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: tiny: 100%|██████████| 4/4 [00:00<00:00, 18.99it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3abcdac4b441cd9d0880afa326dc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 25205.046875\n",
      "Total size (MB): 50538.214926719666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:29:11.920 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-2-7b-hf', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 49,152 || all params: 6,607,400,960 || trainable%: 0.000743893102561162\n",
      "Model size (MB): 0.1875\n",
      "Total size (MB): 25333.551981925964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  25%|██▌       | 1/4 [00:26<01:19, 26.38s/it]2023-11-10 11:29:38.299 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 4,202,496 || all params: 6,611,595,264 || trainable%: 0.06356251149979648\n",
      "Model size (MB): 16.03125\n",
      "Total size (MB): 25365.50095653534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  50%|█████     | 2/4 [00:52<00:52, 26.10s/it]2023-11-10 11:30:04.212 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, encoder_hidden_size=4096, prefix_projection=False)\n",
      "trainable params: 2,629,632 || all params: 6,614,175,744 || trainable%: 0.03975751630708408\n",
      "Model size (MB): 10.03125\n",
      "Total size (MB): 25369.359538078308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  75%|███████▌  | 3/4 [01:18<00:25, 25.94s/it]2023-11-10 11:30:29.967 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 1,118,464 || all params: 6,612,664,576 || trainable%: 0.016913968448654608\n",
      "Model size (MB): 4.2666015625\n",
      "Total size (MB): 25357.847754478455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b: 100%|██████████| 4/4 [01:44<00:00, 26.12s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 413.17676544189453\n",
      "Total size (MB): 826.4355382919312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: bert:   0%|          | 0/4 [00:00<?, ?it/s]2023-11-10 11:30:58.900 | INFO     | peft_trainer:create_peft_config:705 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='dmis-lab/biobert-v1.1', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 9,218 || all params: 108,321,028 || trainable%: 0.0085098896956554\n",
      "Model size (MB): 0.03516387939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: bert:  25%|██▌       | 1/4 [00:00<00:01,  1.81it/s]2023-11-10 11:30:59.452 | INFO     | peft_trainer:create_peft_config:648 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 413.33506870269775\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules={'value', 'query'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})\n",
      "trainable params: 296,450 || all params: 108,615,940 || trainable%: 0.27293415680976474\n",
      "Model size (MB): 1.1308670043945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: bert:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s]2023-11-10 11:30:59.978 | INFO     | peft_trainer:create_peft_config:699 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 415.59956455230713\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 185,858 || all params: 108,792,580 || trainable%: 0.1708370184804883\n",
      "Model size (MB): 0.7089920043945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: bert:  75%|███████▌  | 3/4 [00:01<00:00,  1.92it/s]2023-11-10 11:31:00.482 | INFO     | peft_trainer:create_peft_config:719 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 415.85884189605713\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 223,234 || all params: 108,829,956 || trainable%: 0.20512183244841153\n",
      "Model size (MB): 0.8515701293945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: bert: 100%|██████████| 4/4 [00:02<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 416.15399646759033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_df = get_number_of_trainable_params(model_type_mappings, peft_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vanilla': {'PROMPT_TUNING': {'n_trainable_params': 1191940,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.956251152753809,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 4.5468902587890625,\n",
       "   'peft_model_size_GB': 0.004440322518348694,\n",
       "   'peft_full_model_size_MB': 482.42061138153076,\n",
       "   'peft_full_model_size_GB': 0.47111387830227613},\n",
       "  'LORA': {'n_trainable_params': 294912,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.23659742936803138,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 1.125,\n",
       "   'peft_model_size_GB': 0.0010986328125,\n",
       "   'peft_full_model_size_MB': 480.1683073043823,\n",
       "   'peft_full_model_size_GB': 0.46891436260193586},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 184320,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.14787339335501962,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 0.703125,\n",
       "   'peft_model_size_GB': 0.0006866455078125,\n",
       "   'peft_full_model_size_MB': 480.4276456832886,\n",
       "   'peft_full_model_size_GB': 0.4691676227375865},\n",
       "  'P_TUNING': {'n_trainable_params': 221696,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.1778588314520097,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 0.845703125,\n",
       "   'peft_model_size_GB': 0.0008258819580078125,\n",
       "   'peft_full_model_size_MB': 480.7228002548218,\n",
       "   'peft_full_model_size_GB': 0.4694558596238494}},\n",
       " 'mobile': {'PROMPT_TUNING': {'n_trainable_params': 3332,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.013554129506371783,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.0127105712890625,\n",
       "   'peft_model_size_GB': 1.2412667274475098e-05,\n",
       "   'peft_full_model_size_MB': 94.2606315612793,\n",
       "   'peft_full_model_size_GB': 0.09205139800906181},\n",
       "  'LORA': {'n_trainable_params': 221184,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.8997468729703891,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.84375,\n",
       "   'peft_model_size_GB': 0.000823974609375,\n",
       "   'peft_full_model_size_MB': 96.08671188354492,\n",
       "   'peft_full_model_size_GB': 0.09383467957377434},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 245760,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.9997187477448768,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.9375,\n",
       "   'peft_model_size_GB': 0.00091552734375,\n",
       "   'peft_full_model_size_MB': 97.14512252807617,\n",
       "   'peft_full_model_size_GB': 0.09486828371882439},\n",
       "  'P_TUNING': {'n_trainable_params': 153344,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.6237828436449804,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.5849609375,\n",
       "   'peft_model_size_GB': 0.0005712509155273438,\n",
       "   'peft_full_model_size_MB': 96.47470664978027,\n",
       "   'peft_full_model_size_GB': 0.09421358071267605}},\n",
       " 'distil': {'PROMPT_TUNING': {'n_trainable_params': 10756,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.016350336700495363,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.0410308837890625,\n",
       "   'peft_model_size_GB': 4.006922245025635e-05,\n",
       "   'peft_full_model_size_MB': 251.07848262786865,\n",
       "   'peft_full_model_size_GB': 0.24519383069127798},\n",
       "  'LORA': {'n_trainable_params': 147456,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.2241497999728751,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.5625,\n",
       "   'peft_model_size_GB': 0.00054931640625,\n",
       "   'peft_full_model_size_MB': 252.18468379974365,\n",
       "   'peft_full_model_size_GB': 0.24627410527318716},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 92160,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.14009362498304695,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.3515625,\n",
       "   'peft_model_size_GB': 0.00034332275390625,\n",
       "   'peft_full_model_size_MB': 252.2999792098999,\n",
       "   'peft_full_model_size_GB': 0.24638669844716787},\n",
       "  'P_TUNING': {'n_trainable_params': 221696,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.3370029978758851,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.845703125,\n",
       "   'peft_model_size_GB': 0.0008258819580078125,\n",
       "   'peft_full_model_size_MB': 253.2948408126831,\n",
       "   'peft_full_model_size_GB': 0.24735824298113585}},\n",
       " 'tiny': {'PROMPT_TUNING': {'n_trainable_params': 4372,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.03151045041349178,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.0166778564453125,\n",
       "   'peft_model_size_GB': 1.6286969184875488e-05,\n",
       "   'peft_full_model_size_MB': 52.99952507019043,\n",
       "   'peft_full_model_size_GB': 0.05175734870135784},\n",
       "  'LORA': {'n_trainable_params': 39936,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.2878319642527922,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.15234375,\n",
       "   'peft_model_size_GB': 0.000148773193359375,\n",
       "   'peft_full_model_size_MB': 53.30250358581543,\n",
       "   'peft_full_model_size_GB': 0.05205322615802288},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 24960,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.17989497765799514,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.09521484375,\n",
       "   'peft_model_size_GB': 9.298324584960938e-05,\n",
       "   'peft_full_model_size_MB': 53.33155632019043,\n",
       "   'peft_full_model_size_GB': 0.052081597968935966},\n",
       "  'P_TUNING': {'n_trainable_params': 99944,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.720329473038889,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.381256103515625,\n",
       "   'peft_model_size_GB': 0.00037232041358947754,\n",
       "   'peft_full_model_size_MB': 53.90902328491211,\n",
       "   'peft_full_model_size_GB': 0.05264553055167198}},\n",
       " 'llama-7b': {'PROMPT_TUNING': {'n_trainable_params': 57344,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.000867881742433776,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 0.21875,\n",
       "   'peft_model_size_GB': 0.000213623046875,\n",
       "   'peft_full_model_size_MB': 25333.601351737976,\n",
       "   'peft_full_model_size_GB': 24.739845070056617},\n",
       "  'LORA': {'n_trainable_params': 4194304,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.06347935030372762,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 16.0,\n",
       "   'peft_model_size_GB': 0.015625,\n",
       "   'peft_full_model_size_MB': 25365.488444328308,\n",
       "   'peft_full_model_size_GB': 24.770984808914363},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 2621440,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.03967459393982976,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 10.0,\n",
       "   'peft_model_size_GB': 0.009765625,\n",
       "   'peft_full_model_size_MB': 25369.347331047058,\n",
       "   'peft_full_model_size_GB': 24.774753252975643},\n",
       "  'P_TUNING': {'n_trainable_params': 1110272,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.016803585343461103,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 4.2353515625,\n",
       "   'peft_model_size_GB': 0.004136085510253906,\n",
       "   'peft_full_model_size_MB': 25357.835852622986,\n",
       "   'peft_full_model_size_GB': 24.763511574827135}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open(\"../model_type_trainable_model_size.yaml\", \"w\") as f:\n",
    "    yaml.dump(all_df, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload that yaml file\n",
    "with open(\"../model_type_trainable_model_size.yaml\", \"r\") as f:\n",
    "    all_df = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### model type: bert ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 0.807065955363214\n",
      "full_model_size_MB: 826.4355382919312\n",
      "model_size_GB: 0.40349293500185013\n",
      "model_size_MB: 413.17676544189453\n",
      "n_trainable_params: 296450\n",
      "n_trainable_params_perc: 0.2737005318256615\n",
      "peft_full_model_size_GB: 0.40585894975811243\n",
      "peft_full_model_size_MB: 415.59956455230713\n",
      "peft_model_size_GB: 0.0011043623089790344\n",
      "peft_model_size_MB: 1.1308670043945312\n",
      "total_trainable_params: 108311810\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 0.807065955363214\n",
      "full_model_size_MB: 826.4355382919312\n",
      "model_size_GB: 0.40349293500185013\n",
      "model_size_MB: 413.17676544189453\n",
      "n_trainable_params: 185858\n",
      "n_trainable_params_perc: 0.17159532279997905\n",
      "peft_full_model_size_GB: 0.4061121502891183\n",
      "peft_full_model_size_MB: 415.85884189605713\n",
      "peft_model_size_GB: 0.0006923750042915344\n",
      "peft_model_size_MB: 0.7089920043945312\n",
      "total_trainable_params: 108311810\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 0.807065955363214\n",
      "full_model_size_MB: 826.4355382919312\n",
      "model_size_GB: 0.40349293500185013\n",
      "model_size_MB: 413.17676544189453\n",
      "n_trainable_params: 9218\n",
      "n_trainable_params_perc: 0.008510613939514076\n",
      "peft_full_model_size_GB: 0.4036475280299783\n",
      "peft_full_model_size_MB: 413.33506870269775\n",
      "peft_model_size_GB: 3.4339725971221924e-05\n",
      "peft_model_size_MB: 0.03516387939453125\n",
      "total_trainable_params: 108311810\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 0.807065955363214\n",
      "full_model_size_MB: 826.4355382919312\n",
      "model_size_GB: 0.40349293500185013\n",
      "model_size_MB: 413.17676544189453\n",
      "n_trainable_params: 223234\n",
      "n_trainable_params_perc: 0.20610310177625138\n",
      "peft_full_model_size_GB: 0.4064003871753812\n",
      "peft_full_model_size_MB: 416.15399646759033\n",
      "peft_model_size_GB: 0.0008316114544868469\n",
      "peft_model_size_MB: 0.8515701293945312\n",
      "total_trainable_params: 108311810\n",
      "\n",
      "\n",
      "###### model type: distil ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 0.4901782190427184\n",
      "full_model_size_MB: 501.94249629974365\n",
      "model_size_GB: 0.24506665021181107\n",
      "model_size_MB: 250.94824981689453\n",
      "n_trainable_params: 148994\n",
      "n_trainable_params_perc: 0.22648773394882915\n",
      "peft_full_model_size_GB: 0.24627566430717707\n",
      "peft_full_model_size_MB: 252.18628025054932\n",
      "peft_model_size_GB: 0.0005550459027290344\n",
      "peft_model_size_MB: 0.5683670043945312\n",
      "total_trainable_params: 65784578\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 0.4901782190427184\n",
      "full_model_size_MB: 501.94249629974365\n",
      "model_size_GB: 0.24506665021181107\n",
      "model_size_MB: 250.94824981689453\n",
      "n_trainable_params: 93698\n",
      "n_trainable_params_perc: 0.142431558959001\n",
      "peft_full_model_size_GB: 0.24638825748115778\n",
      "peft_full_model_size_MB: 252.30157566070557\n",
      "peft_model_size_GB: 0.0003490522503852844\n",
      "peft_model_size_MB: 0.35742950439453125\n",
      "total_trainable_params: 65784578\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 0.4901782190427184\n",
      "full_model_size_MB: 501.94249629974365\n",
      "model_size_GB: 0.24506665021181107\n",
      "model_size_MB: 250.94824981689453\n",
      "n_trainable_params: 9218\n",
      "n_trainable_params_perc: 0.014012402724541306\n",
      "peft_full_model_size_GB: 0.2451839903369546\n",
      "peft_full_model_size_MB: 251.0684061050415\n",
      "peft_model_size_GB: 3.4339725971221924e-05\n",
      "peft_model_size_MB: 0.03516387939453125\n",
      "total_trainable_params: 65784578\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 0.4901782190427184\n",
      "full_model_size_MB: 501.94249629974365\n",
      "model_size_GB: 0.24506665021181107\n",
      "model_size_MB: 250.94824981689453\n",
      "n_trainable_params: 223234\n",
      "n_trainable_params_perc: 0.33934093185183917\n",
      "peft_full_model_size_GB: 0.24735980201512575\n",
      "peft_full_model_size_MB: 253.29643726348877\n",
      "peft_model_size_GB: 0.0008316114544868469\n",
      "peft_model_size_MB: 0.8515701293945312\n",
      "total_trainable_params: 65784578\n",
      "\n",
      "\n",
      "###### model type: llama-7b ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 49.35372551437467\n",
      "full_model_size_MB: 50538.214926719666\n",
      "model_size_GB: 24.614303588867188\n",
      "model_size_MB: 25205.046875\n",
      "n_trainable_params: 4202496\n",
      "n_trainable_params_perc: 0.06360333340978959\n",
      "peft_full_model_size_GB: 24.770997027866542\n",
      "peft_full_model_size_MB: 25365.50095653534\n",
      "peft_model_size_GB: 0.015655517578125\n",
      "peft_model_size_MB: 16.03125\n",
      "total_trainable_params: 6607351808\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 49.35372551437467\n",
      "full_model_size_MB: 50538.214926719666\n",
      "model_size_GB: 24.614303588867188\n",
      "model_size_MB: 25205.046875\n",
      "n_trainable_params: 2629632\n",
      "n_trainable_params_perc: 0.03979857704589173\n",
      "peft_full_model_size_GB: 24.774765173904598\n",
      "peft_full_model_size_MB: 25369.359538078308\n",
      "peft_model_size_GB: 0.009796142578125\n",
      "peft_model_size_MB: 10.03125\n",
      "total_trainable_params: 6607351808\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 49.35372551437467\n",
      "full_model_size_MB: 50538.214926719666\n",
      "model_size_GB: 24.614303588867188\n",
      "model_size_MB: 25205.046875\n",
      "n_trainable_params: 49152\n",
      "n_trainable_params_perc: 0.000743898636371808\n",
      "peft_full_model_size_GB: 24.739796857349575\n",
      "peft_full_model_size_MB: 25333.551981925964\n",
      "peft_model_size_GB: 0.00018310546875\n",
      "peft_model_size_MB: 0.1875\n",
      "total_trainable_params: 6607351808\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 49.35372551437467\n",
      "full_model_size_MB: 50538.214926719666\n",
      "model_size_GB: 24.614303588867188\n",
      "model_size_MB: 25205.046875\n",
      "n_trainable_params: 1118464\n",
      "n_trainable_params_perc: 0.01692756844952307\n",
      "peft_full_model_size_GB: 24.763523197732866\n",
      "peft_full_model_size_MB: 25357.847754478455\n",
      "peft_model_size_GB: 0.004166603088378906\n",
      "peft_model_size_MB: 4.2666015625\n",
      "total_trainable_params: 6607351808\n",
      "\n",
      "\n",
      "###### model type: mobile ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 0.18357980996370316\n",
      "full_model_size_MB: 187.98572540283203\n",
      "model_size_GB: 0.09157849103212357\n",
      "model_size_MB: 93.77637481689453\n",
      "n_trainable_params: 222210\n",
      "n_trainable_params_perc: 0.9039205034846561\n",
      "peft_full_model_size_GB: 0.09383438993245363\n",
      "peft_full_model_size_MB: 96.08641529083252\n",
      "peft_model_size_GB: 0.0008277967572212219\n",
      "peft_model_size_MB: 0.8476638793945312\n",
      "total_trainable_params: 24582914\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 0.18357980996370316\n",
      "full_model_size_MB: 187.98572540283203\n",
      "model_size_GB: 0.09157849103212357\n",
      "model_size_MB: 93.77637481689453\n",
      "n_trainable_params: 246786\n",
      "n_trainable_params_perc: 1.0038923782591438\n",
      "peft_full_model_size_GB: 0.0948679344728589\n",
      "peft_full_model_size_MB: 97.14476490020752\n",
      "peft_model_size_GB: 0.0009193494915962219\n",
      "peft_model_size_MB: 0.9414138793945312\n",
      "total_trainable_params: 24582914\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 0.18357980996370316\n",
      "full_model_size_MB: 187.98572540283203\n",
      "model_size_GB: 0.09157849103212357\n",
      "model_size_MB: 93.77637481689453\n",
      "n_trainable_params: 2306\n",
      "n_trainable_params_perc: 0.00938049899210484\n",
      "peft_full_model_size_GB: 0.09204346407204866\n",
      "peft_full_model_size_MB: 94.25250720977783\n",
      "peft_model_size_GB: 8.590519428253174e-06\n",
      "peft_model_size_MB: 0.00879669189453125\n",
      "total_trainable_params: 24582914\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 0.18357980996370316\n",
      "full_model_size_MB: 187.98572540283203\n",
      "model_size_GB: 0.09157849103212357\n",
      "model_size_MB: 93.77637481689453\n",
      "n_trainable_params: 154370\n",
      "n_trainable_params_perc: 0.6279564741592474\n",
      "peft_full_model_size_GB: 0.09421323146671057\n",
      "peft_full_model_size_MB: 96.47434902191162\n",
      "peft_model_size_GB: 0.0005750730633735657\n",
      "peft_model_size_MB: 0.5888748168945312\n",
      "total_trainable_params: 24582914\n",
      "\n",
      "\n",
      "###### model type: roberta ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 0.9287750935181975\n",
      "full_model_size_MB: 951.0656957626343\n",
      "model_size_GB: 0.46434689313173294\n",
      "model_size_MB: 475.49121856689453\n",
      "n_trainable_params: 887042\n",
      "n_trainable_params_perc: 0.7116423100500396\n",
      "peft_full_model_size_GB: 0.4711159886792302\n",
      "peft_full_model_size_MB: 482.42277240753174\n",
      "peft_model_size_GB: 0.003304488956928253\n",
      "peft_model_size_MB: 3.3837966918945312\n",
      "total_trainable_params: 124647170\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 0.9287750935181975\n",
      "full_model_size_MB: 951.0656957626343\n",
      "model_size_GB: 0.46434689313173294\n",
      "model_size_MB: 475.49121856689453\n",
      "n_trainable_params: 776450\n",
      "n_trainable_params_perc: 0.6229182740370278\n",
      "peft_full_model_size_GB: 0.47136924881488085\n",
      "peft_full_model_size_MB: 482.682110786438\n",
      "peft_model_size_GB: 0.002892501652240753\n",
      "peft_model_size_MB: 2.9619216918945312\n",
      "total_trainable_params: 124647170\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 0.9287750935181975\n",
      "full_model_size_MB: 951.0656957626343\n",
      "model_size_GB: 0.46434689313173294\n",
      "model_size_MB: 475.49121856689453\n",
      "n_trainable_params: 599810\n",
      "n_trainable_params_perc: 0.4812062720718008\n",
      "peft_full_model_size_GB: 0.46890385169535875\n",
      "peft_full_model_size_MB: 480.15754413604736\n",
      "peft_model_size_GB: 0.0022344663739204407\n",
      "peft_model_size_MB: 2.2880935668945312\n",
      "total_trainable_params: 124647170\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 0.9287750935181975\n",
      "full_model_size_MB: 951.0656957626343\n",
      "model_size_GB: 0.46434689313173294\n",
      "model_size_MB: 475.49121856689453\n",
      "n_trainable_params: 813826\n",
      "n_trainable_params_perc: 0.652903712134018\n",
      "peft_full_model_size_GB: 0.47165748570114374\n",
      "peft_full_model_size_MB: 482.9772653579712\n",
      "peft_model_size_GB: 0.0030317381024360657\n",
      "peft_model_size_MB: 3.1044998168945312\n",
      "total_trainable_params: 124647170\n",
      "\n",
      "\n",
      "###### model type: tiny ######### \n",
      "\n",
      "peft method: LORA\n",
      "full_model_size_GB: 0.10340742953121662\n",
      "full_model_size_MB: 105.88920783996582\n",
      "model_size_GB: 0.051687516272068024\n",
      "model_size_MB: 52.928016662597656\n",
      "n_trainable_params: 40562\n",
      "n_trainable_params_perc: 0.2923437533559134\n",
      "peft_full_model_size_GB: 0.05205115024000406\n",
      "peft_full_model_size_MB: 53.30037784576416\n",
      "peft_model_size_GB: 0.00015110522508621216\n",
      "peft_model_size_MB: 0.15473175048828125\n",
      "total_trainable_params: 13874762\n",
      "\n",
      "\n",
      "peft method: PREFIX_TUNING\n",
      "full_model_size_GB: 0.10340742953121662\n",
      "full_model_size_MB: 105.88920783996582\n",
      "model_size_GB: 0.051687516272068024\n",
      "model_size_MB: 52.928016662597656\n",
      "n_trainable_params: 25586\n",
      "n_trainable_params_perc: 0.18440676676111634\n",
      "peft_full_model_size_GB: 0.05207946244627237\n",
      "peft_full_model_size_MB: 53.32936954498291\n",
      "peft_model_size_GB: 9.531527757644653e-05\n",
      "peft_model_size_MB: 0.09760284423828125\n",
      "total_trainable_params: 13874762\n",
      "\n",
      "\n",
      "peft method: PROMPT_TUNING\n",
      "full_model_size_GB: 0.10340742953121662\n",
      "full_model_size_MB: 105.88920783996582\n",
      "model_size_GB: 0.051687516272068024\n",
      "model_size_MB: 52.928016662597656\n",
      "n_trainable_params: 3746\n",
      "n_trainable_params_perc: 0.026998661310370584\n",
      "peft_full_model_size_GB: 0.051750787533819675\n",
      "peft_full_model_size_MB: 52.99280643463135\n",
      "peft_model_size_GB: 1.395493745803833e-05\n",
      "peft_model_size_MB: 0.01428985595703125\n",
      "total_trainable_params: 13874762\n",
      "\n",
      "\n",
      "peft method: P_TUNING\n",
      "full_model_size_GB: 0.10340742953121662\n",
      "full_model_size_MB: 105.88920783996582\n",
      "model_size_GB: 0.051687516272068024\n",
      "model_size_MB: 52.928016662597656\n",
      "n_trainable_params: 100570\n",
      "n_trainable_params_perc: 0.7248412621420101\n",
      "peft_full_model_size_GB: 0.05264339502900839\n",
      "peft_full_model_size_MB: 53.90683650970459\n",
      "peft_model_size_GB: 0.0003746524453163147\n",
      "peft_model_size_MB: 0.38364410400390625\n",
      "total_trainable_params: 13874762\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run through they dict keys and nicely print the attributes and values\n",
    "for model_type in all_df.keys():\n",
    "    print(f\"###### model type: {model_type} ######### \\n\")\n",
    "    for peft_method in all_df[model_type].keys():\n",
    "        print(f\"peft method: {peft_method}\")\n",
    "        for attribute in all_df[model_type][peft_method].keys():\n",
    "            print(f\"{attribute}: {all_df[model_type][peft_method][attribute]}\")\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
