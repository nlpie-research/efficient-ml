{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (LoraConfig,IA3Config, PeftType, PrefixTuningConfig,\n",
    "                  PromptEncoderConfig, PromptTuningConfig, TaskType,\n",
    "                  get_peft_config, get_peft_model, get_peft_model_state_dict,\n",
    "                  prepare_model_for_int8_training,\n",
    "                  prepare_model_for_kbit_training, set_peft_model_state_dict)\n",
    "from scipy.special import softmax\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForTokenClassification, AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          LlamaForSequenceClassification, LlamaTokenizer,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          get_linear_schedule_with_warmup, set_seed)\n",
    "from model_utils import count_trainable_parameters, get_model_size, get_full_model_size\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "# import sys and append path\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from peft_trainer import create_peft_config\n",
    "from loguru import logger as loguru_logger\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arg_dict = {\n",
    "  \"log_save_dir\": \"/mnt/sdc/niallt/saved_models/peft_training/logs\",\n",
    "  \"task_type\": \"SEQ_CLS\",\n",
    "  \"peft_method\": \"LORA\",\n",
    "  \"lora_rank\": 8, \n",
    "  \"lora_alpha\": 16,\n",
    "  \"lora_dropout\": 0.1,\n",
    "  \"learning_rate\": 3e-4,\n",
    "  \"num_virtual_tokens\": 10 \n",
    "}\n",
    "\n",
    "# convert above args to a namespace object\n",
    "\n",
    "args = Namespace(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_type_mappings = {\"vanilla\": \"roberta-base\",\n",
    "               \"mobile\": \"nlpie/bio-mobilebert\",\n",
    "               \"distil\": \"nlpie/distil-biobert\",\n",
    "               \"tiny\": \"nlpie/tiny-biobert\",\n",
    "               \"llama-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "               }\n",
    "\n",
    "peft_types = [\"PROMPT_TUNING\",\"LORA\", \"PREFIX_TUNING\", \"P_TUNING\"]\n",
    "\n",
    "def get_number_of_trainable_params(model_type_mappings:dict,\n",
    "                                   peft_types:list,\n",
    "                                   task_type:str = \"SEQ_CLS\",\n",
    "                                   num_labels:int = 2):\n",
    "\n",
    "    # set up empty dicts to full for dfs\n",
    "    model_peft_dict = {}\n",
    "    \n",
    "    for model_type in model_type_mappings.keys():\n",
    "        \n",
    "        model_dict = {}\n",
    "        model_name_or_path = model_type_mappings[model_type]\n",
    "        model_args = dict(pretrained_model_name_or_path=model_name_or_path, \n",
    "                          num_labels=num_labels, \n",
    "                          output_hidden_states=False, \n",
    "                          trust_remote_code=True)\n",
    "\n",
    "            \n",
    "        if task_type == \"SEQ_CLS\":\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(**model_args)\n",
    "        elif task_type == \"TOKEN_CLS\":\n",
    "            model = AutoModelForTokenClassification.from_pretrained(**model_args)\n",
    "        \n",
    "        # falcon model seems to use model config to define pad token and the remote code panicks if you don't set it\n",
    "        if \"falcon\" in model_name_or_path:\n",
    "            model.config.use_cache = False            \n",
    "\n",
    "        # count total trainable params before peft\n",
    "        total_trainable_params = count_trainable_parameters(model)\n",
    "        \n",
    "        # get model size and full model size too\n",
    "        model_size_MB, model_size_GB = get_model_size(model)\n",
    "        full_model_size_MB, full_model_size_GB = get_full_model_size(model)\n",
    "        \n",
    "        for peft_method in tqdm(peft_types, desc=f\"model type: {model_type}\"):\n",
    "            \n",
    "            \n",
    "            # set up some PEFT params\n",
    "            peft_config, lr = create_peft_config(args, peft_method, model_name_or_path,task_type)\n",
    "            model = get_peft_model(model, peft_config)\n",
    "            print(f\"peft config is: {peft_config}\")\n",
    "            # print(model)\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            # lets also confirm this directly and save to args\n",
    "            n_trainable_params = count_trainable_parameters(model)\n",
    "            # proportion of total trainable params\n",
    "            n_trainable_params_perc = (n_trainable_params / total_trainable_params) * 100\n",
    "            \n",
    "            # get size of peft adapter only\n",
    "            peft_model_size_MB, peft_model_size_GB = get_model_size(model)\n",
    "            peft_full_model_size_MB, peft_full_model_size_GB = get_full_model_size(model)\n",
    "            \n",
    "            # store the model name, peft method and number of trainable params\n",
    "            model_dict[peft_method] = {\"n_trainable_params\": n_trainable_params,\n",
    "                                 \"total_trainable_params\": total_trainable_params,\n",
    "                                 \"n_trainable_params_perc\": n_trainable_params_perc,\n",
    "                                 \"model_size_MB\": model_size_MB,\n",
    "                                 \"model_size_GB\": model_size_GB,\n",
    "                                 \"full_model_size_MB\": full_model_size_MB,\n",
    "                                 \"full_model_size_GB\": full_model_size_GB,\n",
    "                                 \"peft_model_size_MB\": peft_model_size_MB,\n",
    "                                 \"peft_model_size_GB\": peft_model_size_GB,\n",
    "                                 \"peft_full_model_size_MB\": peft_full_model_size_MB,\n",
    "                                 \"peft_full_model_size_GB\": peft_full_model_size_GB,}\n",
    "            \n",
    "        model_peft_dict[model_type] = model_dict\n",
    "\n",
    "    return model_peft_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 475.49121856689453\n",
      "Total size (MB): 951.0699663162231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla:   0%|          | 0/4 [00:00<?, ?it/s]2023-10-06 11:29:05.025 | INFO     | peft_trainer:create_peft_config:672 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='roberta-base', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 1,191,940 || all params: 125,246,980 || trainable%: 0.9516716490888643\n",
      "Model size (MB): 4.5468902587890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla:  25%|██▌       | 1/4 [00:00<00:01,  1.67it/s]2023-10-06 11:29:05.626 | INFO     | peft_trainer:create_peft_config:615 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 482.42061138153076\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 294,912 || all params: 125,541,892 || trainable%: 0.23491122787921662\n",
      "Model size (MB): 1.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla:  50%|█████     | 2/4 [00:01<00:01,  1.42it/s]2023-10-06 11:29:06.406 | INFO     | peft_trainer:create_peft_config:666 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 480.1683073043823\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 184,320 || all params: 125,718,532 || trainable%: 0.1466132296231394\n",
      "Model size (MB): 0.703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla:  75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]2023-10-06 11:29:07.041 | INFO     | peft_trainer:create_peft_config:686 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 480.4276456832886\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 221,696 || all params: 125,755,908 || trainable%: 0.17629072345451952\n",
      "Model size (MB): 0.845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 480.7228002548218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at nlpie/bio-mobilebert were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 93.77637481689453\n",
      "Total size (MB): 187.98993587493896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:   0%|          | 0/4 [00:00<?, ?it/s]2023-10-06 11:29:09.659 | INFO     | peft_trainer:create_peft_config:672 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/bio-mobilebert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=128, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 3,332 || all params: 24,585,220 || trainable%: 0.013552858180646747\n",
      "Model size (MB): 0.0127105712890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  25%|██▌       | 1/4 [00:00<00:00,  5.43it/s]2023-10-06 11:29:09.845 | INFO     | peft_trainer:create_peft_config:615 - Using LORA\n",
      "2023-10-06 11:29:09.845 | INFO     | peft_trainer:create_peft_config:640 - Using mobile config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 94.2606315612793\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'key', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 221,184 || all params: 24,806,404 || trainable%: 0.8916407230971486\n",
      "Model size (MB): 0.84375\n",
      "Total size (MB): 96.08671188354492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  50%|█████     | 2/4 [00:00<00:00,  3.61it/s]2023-10-06 11:29:10.187 | INFO     | peft_trainer:create_peft_config:666 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_hidden_size=512, prefix_projection=False)\n",
      "trainable params: 245,760 || all params: 25,050,884 || trainable%: 0.9810432238638764\n",
      "Model size (MB): 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile:  75%|███████▌  | 3/4 [00:00<00:00,  3.88it/s]2023-10-06 11:29:10.422 | INFO     | peft_trainer:create_peft_config:686 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 97.14512252807617\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 153,344 || all params: 24,958,468 || trainable%: 0.6143966849247318\n",
      "Model size (MB): 0.5849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: mobile: 100%|██████████| 4/4 [00:00<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 96.47470664978027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at nlpie/distil-biobert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/distil-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 250.94824981689453\n",
      "Total size (MB): 501.9467668533325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:   0%|          | 0/4 [00:00<?, ?it/s]2023-10-06 11:29:11.716 | INFO     | peft_trainer:create_peft_config:672 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/distil-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 10,756 || all params: 65,793,796 || trainable%: 0.016348045946459753\n",
      "Model size (MB): 0.0410308837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  25%|██▌       | 1/4 [00:00<00:01,  2.96it/s]2023-10-06 11:29:12.056 | INFO     | peft_trainer:create_peft_config:615 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 251.07848262786865\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 147,456 || all params: 65,941,252 || trainable%: 0.22361722825644864\n",
      "Model size (MB): 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  50%|█████     | 2/4 [00:00<00:00,  2.37it/s]2023-10-06 11:29:12.535 | INFO     | peft_trainer:create_peft_config:666 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 252.18468379974365\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 92,160 || all params: 66,025,732 || trainable%: 0.13958194359738413\n",
      "Model size (MB): 0.3515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil:  75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]2023-10-06 11:29:12.842 | INFO     | peft_trainer:create_peft_config:686 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 252.2999792098999\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 221,696 || all params: 66,155,268 || trainable%: 0.335114657837982\n",
      "Model size (MB): 0.845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: distil: 100%|██████████| 4/4 [00:01<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 253.2948408126831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at nlpie/tiny-biobert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 52.928016662597656\n",
      "Total size (MB): 105.89353847503662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: tiny:   0%|          | 0/4 [00:00<?, ?it/s]2023-10-06 11:29:13.502 | INFO     | peft_trainer:create_peft_config:672 - Using PROMPT_TUNING\n",
      "2023-10-06 11:29:13.575 | INFO     | peft_trainer:create_peft_config:615 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/tiny-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 4,372 || all params: 13,878,508 || trainable%: 0.03150194530997136\n",
      "Model size (MB): 0.0166778564453125\n",
      "Total size (MB): 52.99952507019043\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 39,936 || all params: 13,918,444 || trainable%: 0.2869286250675722\n",
      "Model size (MB): 0.15234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: tiny:  50%|█████     | 2/4 [00:00<00:00, 12.49it/s]2023-10-06 11:29:13.663 | INFO     | peft_trainer:create_peft_config:666 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size (MB): 53.30250358581543\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_hidden_size=312, prefix_projection=False)\n",
      "trainable params: 24,960 || all params: 13,940,284 || trainable%: 0.17904943686943536\n",
      "Model size (MB): 0.09521484375\n",
      "Total size (MB): 53.33155632019043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:29:13.719 | INFO     | peft_trainer:create_peft_config:686 - Using P_TUNING\n",
      "model type: tiny: 100%|██████████| 4/4 [00:00<00:00, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 99,944 || all params: 14,015,268 || trainable%: 0.7131080190546482\n",
      "Model size (MB): 0.381256103515625\n",
      "Total size (MB): 53.90902328491211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4f140506e646e69d5cb85b7d98c96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 25205.046875\n",
      "Total size (MB): 50538.23268032074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:   0%|          | 0/4 [00:00<?, ?it/s]2023-10-06 11:30:44.168 | INFO     | peft_trainer:create_peft_config:672 - Using PROMPT_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-2-7b-hf', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 57,344 || all params: 6,607,400,960 || trainable%: 0.0008678752863213557\n",
      "Model size (MB): 0.21875\n",
      "Total size (MB): 25333.601351737976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  25%|██▌       | 1/4 [00:29<01:29, 29.96s/it]2023-10-06 11:31:14.125 | INFO     | peft_trainer:create_peft_config:615 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['q_proj', 'v_proj'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 4,194,304 || all params: 6,611,595,264 || trainable%: 0.0634386079686078\n",
      "Model size (MB): 16.0\n",
      "Total size (MB): 25365.488444328308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  50%|█████     | 2/4 [01:18<01:22, 41.07s/it]2023-10-06 11:32:02.976 | INFO     | peft_trainer:create_peft_config:666 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, encoder_hidden_size=4096, prefix_projection=False)\n",
      "trainable params: 2,621,440 || all params: 6,614,175,744 || trainable%: 0.039633661116096286\n",
      "Model size (MB): 10.0\n",
      "Total size (MB): 25369.347331047058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b:  75%|███████▌  | 3/4 [01:49<00:36, 36.36s/it]2023-10-06 11:32:33.734 | INFO     | peft_trainer:create_peft_config:686 - Using P_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=4096, num_transformer_submodules=1, num_attention_heads=32, num_layers=32, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 1,110,272 || all params: 6,612,664,576 || trainable%: 0.016790084953493944\n",
      "Model size (MB): 4.2353515625\n",
      "Total size (MB): 25357.835852622986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: llama-7b: 100%|██████████| 4/4 [02:21<00:00, 35.47s/it]\n"
     ]
    }
   ],
   "source": [
    "all_df = get_number_of_trainable_params(model_type_mappings, peft_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vanilla': {'PROMPT_TUNING': {'n_trainable_params': 1191940,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.956251152753809,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 4.5468902587890625,\n",
       "   'peft_model_size_GB': 0.004440322518348694,\n",
       "   'peft_full_model_size_MB': 482.42061138153076,\n",
       "   'peft_full_model_size_GB': 0.47111387830227613},\n",
       "  'LORA': {'n_trainable_params': 294912,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.23659742936803138,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 1.125,\n",
       "   'peft_model_size_GB': 0.0010986328125,\n",
       "   'peft_full_model_size_MB': 480.1683073043823,\n",
       "   'peft_full_model_size_GB': 0.46891436260193586},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 184320,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.14787339335501962,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 0.703125,\n",
       "   'peft_model_size_GB': 0.0006866455078125,\n",
       "   'peft_full_model_size_MB': 480.4276456832886,\n",
       "   'peft_full_model_size_GB': 0.4691676227375865},\n",
       "  'P_TUNING': {'n_trainable_params': 221696,\n",
       "   'total_trainable_params': 124647170,\n",
       "   'n_trainable_params_perc': 0.1778588314520097,\n",
       "   'model_size_MB': 475.49121856689453,\n",
       "   'model_size_GB': 0.46434689313173294,\n",
       "   'full_model_size_MB': 951.0699663162231,\n",
       "   'full_model_size_GB': 0.9287792639806867,\n",
       "   'peft_model_size_MB': 0.845703125,\n",
       "   'peft_model_size_GB': 0.0008258819580078125,\n",
       "   'peft_full_model_size_MB': 480.7228002548218,\n",
       "   'peft_full_model_size_GB': 0.4694558596238494}},\n",
       " 'mobile': {'PROMPT_TUNING': {'n_trainable_params': 3332,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.013554129506371783,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.0127105712890625,\n",
       "   'peft_model_size_GB': 1.2412667274475098e-05,\n",
       "   'peft_full_model_size_MB': 94.2606315612793,\n",
       "   'peft_full_model_size_GB': 0.09205139800906181},\n",
       "  'LORA': {'n_trainable_params': 221184,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.8997468729703891,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.84375,\n",
       "   'peft_model_size_GB': 0.000823974609375,\n",
       "   'peft_full_model_size_MB': 96.08671188354492,\n",
       "   'peft_full_model_size_GB': 0.09383467957377434},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 245760,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.9997187477448768,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.9375,\n",
       "   'peft_model_size_GB': 0.00091552734375,\n",
       "   'peft_full_model_size_MB': 97.14512252807617,\n",
       "   'peft_full_model_size_GB': 0.09486828371882439},\n",
       "  'P_TUNING': {'n_trainable_params': 153344,\n",
       "   'total_trainable_params': 24582914,\n",
       "   'n_trainable_params_perc': 0.6237828436449804,\n",
       "   'model_size_MB': 93.77637481689453,\n",
       "   'model_size_GB': 0.09157849103212357,\n",
       "   'full_model_size_MB': 187.98993587493896,\n",
       "   'full_model_size_GB': 0.18358392175287008,\n",
       "   'peft_model_size_MB': 0.5849609375,\n",
       "   'peft_model_size_GB': 0.0005712509155273438,\n",
       "   'peft_full_model_size_MB': 96.47470664978027,\n",
       "   'peft_full_model_size_GB': 0.09421358071267605}},\n",
       " 'distil': {'PROMPT_TUNING': {'n_trainable_params': 10756,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.016350336700495363,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.0410308837890625,\n",
       "   'peft_model_size_GB': 4.006922245025635e-05,\n",
       "   'peft_full_model_size_MB': 251.07848262786865,\n",
       "   'peft_full_model_size_GB': 0.24519383069127798},\n",
       "  'LORA': {'n_trainable_params': 147456,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.2241497999728751,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.5625,\n",
       "   'peft_model_size_GB': 0.00054931640625,\n",
       "   'peft_full_model_size_MB': 252.18468379974365,\n",
       "   'peft_full_model_size_GB': 0.24627410527318716},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 92160,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.14009362498304695,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.3515625,\n",
       "   'peft_model_size_GB': 0.00034332275390625,\n",
       "   'peft_full_model_size_MB': 252.2999792098999,\n",
       "   'peft_full_model_size_GB': 0.24638669844716787},\n",
       "  'P_TUNING': {'n_trainable_params': 221696,\n",
       "   'total_trainable_params': 65784578,\n",
       "   'n_trainable_params_perc': 0.3370029978758851,\n",
       "   'model_size_MB': 250.94824981689453,\n",
       "   'model_size_GB': 0.24506665021181107,\n",
       "   'full_model_size_MB': 501.9467668533325,\n",
       "   'full_model_size_GB': 0.49018238950520754,\n",
       "   'peft_model_size_MB': 0.845703125,\n",
       "   'peft_model_size_GB': 0.0008258819580078125,\n",
       "   'peft_full_model_size_MB': 253.2948408126831,\n",
       "   'peft_full_model_size_GB': 0.24735824298113585}},\n",
       " 'tiny': {'PROMPT_TUNING': {'n_trainable_params': 4372,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.03151045041349178,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.0166778564453125,\n",
       "   'peft_model_size_GB': 1.6286969184875488e-05,\n",
       "   'peft_full_model_size_MB': 52.99952507019043,\n",
       "   'peft_full_model_size_GB': 0.05175734870135784},\n",
       "  'LORA': {'n_trainable_params': 39936,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.2878319642527922,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.15234375,\n",
       "   'peft_model_size_GB': 0.000148773193359375,\n",
       "   'peft_full_model_size_MB': 53.30250358581543,\n",
       "   'peft_full_model_size_GB': 0.05205322615802288},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 24960,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.17989497765799514,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.09521484375,\n",
       "   'peft_model_size_GB': 9.298324584960938e-05,\n",
       "   'peft_full_model_size_MB': 53.33155632019043,\n",
       "   'peft_full_model_size_GB': 0.052081597968935966},\n",
       "  'P_TUNING': {'n_trainable_params': 99944,\n",
       "   'total_trainable_params': 13874762,\n",
       "   'n_trainable_params_perc': 0.720329473038889,\n",
       "   'model_size_MB': 52.928016662597656,\n",
       "   'model_size_GB': 0.051687516272068024,\n",
       "   'full_model_size_MB': 105.89353847503662,\n",
       "   'full_model_size_GB': 0.10341165866702795,\n",
       "   'peft_model_size_MB': 0.381256103515625,\n",
       "   'peft_model_size_GB': 0.00037232041358947754,\n",
       "   'peft_full_model_size_MB': 53.90902328491211,\n",
       "   'peft_full_model_size_GB': 0.05264553055167198}},\n",
       " 'llama-7b': {'PROMPT_TUNING': {'n_trainable_params': 57344,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.000867881742433776,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 0.21875,\n",
       "   'peft_model_size_GB': 0.000213623046875,\n",
       "   'peft_full_model_size_MB': 25333.601351737976,\n",
       "   'peft_full_model_size_GB': 24.739845070056617},\n",
       "  'LORA': {'n_trainable_params': 4194304,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.06347935030372762,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 16.0,\n",
       "   'peft_model_size_GB': 0.015625,\n",
       "   'peft_full_model_size_MB': 25365.488444328308,\n",
       "   'peft_full_model_size_GB': 24.770984808914363},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': 2621440,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.03967459393982976,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 10.0,\n",
       "   'peft_model_size_GB': 0.009765625,\n",
       "   'peft_full_model_size_MB': 25369.347331047058,\n",
       "   'peft_full_model_size_GB': 24.774753252975643},\n",
       "  'P_TUNING': {'n_trainable_params': 1110272,\n",
       "   'total_trainable_params': 6607351808,\n",
       "   'n_trainable_params_perc': 0.016803585343461103,\n",
       "   'model_size_MB': 25205.046875,\n",
       "   'model_size_GB': 24.614303588867188,\n",
       "   'full_model_size_MB': 50538.23268032074,\n",
       "   'full_model_size_GB': 49.35374285187572,\n",
       "   'peft_model_size_MB': 4.2353515625,\n",
       "   'peft_model_size_GB': 0.004136085510253906,\n",
       "   'peft_full_model_size_MB': 25357.835852622986,\n",
       "   'peft_full_model_size_GB': 24.763511574827135}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open(\"../model_type_trainable_model_size.yaml\", \"w\") as f:\n",
    "    yaml.dump(all_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
