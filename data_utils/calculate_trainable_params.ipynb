{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import (LoraConfig, PeftType, PrefixTuningConfig,\n",
    "                  PromptEncoderConfig, PromptTuningConfig, TaskType,\n",
    "                  get_peft_config, get_peft_model, get_peft_model_state_dict,\n",
    "                  prepare_model_for_int8_training,\n",
    "                  prepare_model_for_kbit_training, set_peft_model_state_dict)\n",
    "from scipy.special import softmax\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForTokenClassification, AutoTokenizer,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          LlamaForSequenceClassification, LlamaTokenizer,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          get_linear_schedule_with_warmup, set_seed)\n",
    "from model_utils import count_trainable_parameters\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "# import sys and append path\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from peft_trainer import create_peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2b4095615e45e89910e9e48496b46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at meta-llama/Llama-2-7b-hf were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_type_mappings = {\"vanilla\": \"roberta-base\",\n",
    "               \"mobile\": \"nlpie/bio-mobilebert\",\n",
    "               \"distil\": \"nlpie/distil-biobert\",\n",
    "               \"tiny\": \"nlpie/tiny-biobert\",\n",
    "            #    \"llama-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "               }\n",
    "\n",
    "peft_types = [\"PROMPT_TUNING\",\"LORA\", \"PREFIX_TUNING\", \"P_TUNING\"]\n",
    "\n",
    "def get_number_of_trainable_params(model_type_mappings:dict,\n",
    "                                   peft_types:list,\n",
    "                                   task_type:str = \"SEQ_CLS\",\n",
    "                                   num_labels:int = 2):\n",
    "\n",
    "    # set up empty dicts to full for dfs\n",
    "    model_peft_dict = {}\n",
    "    \n",
    "    for model_type in model_type_mappings.keys():\n",
    "        \n",
    "        model_dict = {}\n",
    "        model_name_or_path = model_type_mappings[model_type]\n",
    "        model_args = dict(pretrained_model_name_or_path=model_name_or_path, \n",
    "                          num_labels=num_labels, \n",
    "                          output_hidden_states=False, \n",
    "                          trust_remote_code=True)\n",
    "\n",
    "            \n",
    "        if task_type == \"SEQ_CLS\":\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(**model_args)\n",
    "        elif task_type == \"TOKEN_CLS\":\n",
    "            model = AutoModelForTokenClassification.from_pretrained(**model_args)\n",
    "        \n",
    "        # falcon model seems to use model config to define pad token and the remote code panicks if you don't set it\n",
    "        if \"falcon\" in model_name_or_path:\n",
    "            model.config.use_cache = False            \n",
    "\n",
    "        # count total trainable params before peft\n",
    "        total_trainable_params = count_trainable_parameters(model)\n",
    "        \n",
    "        for peft_method in tqdm(peft_types, desc=f\"model type: {model_type}\"):\n",
    "            \n",
    "            \n",
    "            # set up some PEFT params\n",
    "            peft_config, lr = create_peft_config(peft_method, model_name_or_path,task_type)\n",
    "            model = get_peft_model(model, peft_config)\n",
    "            print(f\"peft config is: {peft_config}\")\n",
    "            # print(model)\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            # lets also confirm this directly and save to args\n",
    "            n_trainable_params = count_trainable_parameters(model)\n",
    "            # proportion of total trainable params\n",
    "            n_trainable_params_perc = (n_trainable_params / total_trainable_params) * 100\n",
    "            \n",
    "            # store the model name, peft method and number of trainable params\n",
    "            model_dict[peft_method] = {\"n_trainable_params\": [n_trainable_params],\n",
    "                                 \"total_trainable_params\": [total_trainable_params],\n",
    "                                 \"n_trainable_params_perc\": [n_trainable_params_perc]}\n",
    "            \n",
    "        model_peft_dict[model_type] = model_dict\n",
    "\n",
    "    return model_peft_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model type: vanilla:   0%|          | 0/4 [00:00<?, ?it/s]2023-08-16 14:12:11.296 | INFO     | peft_trainer:create_peft_config:647 - Using PROMPT_TUNING\n",
      "2023-08-16 14:12:11.311 | INFO     | peft_trainer:create_peft_config:593 - Using LORA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='roberta-base', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 1,191,940 || all params: 125,246,980 || trainable%: 0.9516716490888643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model type: vanilla:  50%|█████     | 2/4 [00:00<00:00,  7.00it/s]2023-08-16 14:12:11.583 | INFO     | peft_trainer:create_peft_config:641 - Using PREFIX_TUNING\n",
      "2023-08-16 14:12:11.615 | INFO     | peft_trainer:create_peft_config:661 - Using P_TUNING\n",
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/peft/tuners/p_tuning.py:146: UserWarning: for MLP, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\n",
      "  warnings.warn(\n",
      "model type: vanilla: 100%|██████████| 4/4 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 294,912 || all params: 125,541,892 || trainable%: 0.23491122787921662\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 368,640 || all params: 125,902,852 || trainable%: 0.2927971798446631\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 229,376 || all params: 125,763,588 || trainable%: 0.18238665391766654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at nlpie/bio-mobilebert were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model type: mobile:   0%|          | 0/4 [00:00<?, ?it/s]2023-08-16 14:12:13.806 | INFO     | peft_trainer:create_peft_config:647 - Using PROMPT_TUNING\n",
      "2023-08-16 14:12:13.870 | INFO     | peft_trainer:create_peft_config:593 - Using LORA\n",
      "2023-08-16 14:12:13.872 | INFO     | peft_trainer:create_peft_config:618 - Using mobile config\n",
      "model type: mobile:  50%|█████     | 2/4 [00:00<00:00,  8.78it/s]2023-08-16 14:12:14.035 | INFO     | peft_trainer:create_peft_config:641 - Using PREFIX_TUNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/bio-mobilebert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=128, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 3,332 || all params: 24,585,220 || trainable%: 0.013552858180646747\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'key', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 221,184 || all params: 24,806,404 || trainable%: 0.8916407230971486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 14:12:14.097 | INFO     | peft_trainer:create_peft_config:661 - Using P_TUNING\n",
      "model type: mobile: 100%|██████████| 4/4 [00:00<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_hidden_size=512, prefix_projection=False)\n",
      "trainable params: 491,520 || all params: 25,296,644 || trainable%: 1.9430245371678552\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=512, num_transformer_submodules=1, num_attention_heads=4, num_layers=24, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 158,464 || all params: 24,963,588 || trainable%: 0.6347805451684269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpie/distil-biobert were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/distil-biobert and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model type: distil:   0%|          | 0/4 [00:00<?, ?it/s]2023-08-16 14:12:15.587 | INFO     | peft_trainer:create_peft_config:647 - Using PROMPT_TUNING\n",
      "2023-08-16 14:12:15.592 | INFO     | peft_trainer:create_peft_config:593 - Using LORA\n",
      "model type: distil:  50%|█████     | 2/4 [00:00<00:00, 12.80it/s]2023-08-16 14:12:15.745 | INFO     | peft_trainer:create_peft_config:641 - Using PREFIX_TUNING\n",
      "2023-08-16 14:12:15.756 | INFO     | peft_trainer:create_peft_config:661 - Using P_TUNING\n",
      "model type: distil: 100%|██████████| 4/4 [00:00<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/distil-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 10,756 || all params: 65,793,796 || trainable%: 0.016348045946459753\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 147,456 || all params: 65,941,252 || trainable%: 0.22361722825644864\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_hidden_size=768, prefix_projection=False)\n",
      "trainable params: 184,320 || all params: 66,117,892 || trainable%: 0.278774767955397\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=768, num_transformer_submodules=1, num_attention_heads=12, num_layers=6, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 229,376 || all params: 66,162,948 || trainable%: 0.34668346398349725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpie/tiny-biobert were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model type: tiny:   0%|          | 0/4 [00:00<?, ?it/s]2023-08-16 14:12:16.106 | INFO     | peft_trainer:create_peft_config:647 - Using PROMPT_TUNING\n",
      "2023-08-16 14:12:16.111 | INFO     | peft_trainer:create_peft_config:593 - Using LORA\n",
      "2023-08-16 14:12:16.161 | INFO     | peft_trainer:create_peft_config:641 - Using PREFIX_TUNING\n",
      "2023-08-16 14:12:16.173 | INFO     | peft_trainer:create_peft_config:661 - Using P_TUNING\n",
      "model type: tiny: 100%|██████████| 4/4 [00:00<00:00, 49.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config is: PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='nlpie/tiny-biobert', revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=10, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None)\n",
      "trainable params: 4,372 || all params: 13,878,508 || trainable%: 0.03150194530997136\n",
      "peft config is: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
      "trainable params: 39,936 || all params: 13,918,444 || trainable%: 0.2869286250675722\n",
      "peft config is: PrefixTuningConfig(peft_type=<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_hidden_size=312, prefix_projection=False)\n",
      "trainable params: 49,920 || all params: 13,965,244 || trainable%: 0.3574588456886253\n",
      "peft config is: PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, num_virtual_tokens=20, token_dim=312, num_transformer_submodules=1, num_attention_heads=12, num_layers=4, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=128, encoder_num_layers=2, encoder_dropout=0.0)\n",
      "trainable params: 103,064 || all params: 14,018,388 || trainable%: 0.7352057882832177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_df = get_number_of_trainable_params(model_type_mappings, peft_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vanilla': {'PROMPT_TUNING': {'n_trainable_params': [1191940],\n",
       "   'total_trainable_params': [124647170],\n",
       "   'n_trainable_params_perc': [0.956251152753809]},\n",
       "  'LORA': {'n_trainable_params': [294912],\n",
       "   'total_trainable_params': [124647170],\n",
       "   'n_trainable_params_perc': [0.23659742936803138]},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': [368640],\n",
       "   'total_trainable_params': [124647170],\n",
       "   'n_trainable_params_perc': [0.29574678671003923]},\n",
       "  'P_TUNING': {'n_trainable_params': [229376],\n",
       "   'total_trainable_params': [124647170],\n",
       "   'n_trainable_params_perc': [0.1840202228418022]}},\n",
       " 'mobile': {'PROMPT_TUNING': {'n_trainable_params': [3332],\n",
       "   'total_trainable_params': [24582914],\n",
       "   'n_trainable_params_perc': [0.013554129506371783]},\n",
       "  'LORA': {'n_trainable_params': [221184],\n",
       "   'total_trainable_params': [24582914],\n",
       "   'n_trainable_params_perc': [0.8997468729703891]},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': [491520],\n",
       "   'total_trainable_params': [24582914],\n",
       "   'n_trainable_params_perc': [1.9994374954897536]},\n",
       "  'P_TUNING': {'n_trainable_params': [158464],\n",
       "   'total_trainable_params': [24582914],\n",
       "   'n_trainable_params_perc': [0.644610317556332]}},\n",
       " 'distil': {'PROMPT_TUNING': {'n_trainable_params': [10756],\n",
       "   'total_trainable_params': [65784578],\n",
       "   'n_trainable_params_perc': [0.016350336700495363]},\n",
       "  'LORA': {'n_trainable_params': [147456],\n",
       "   'total_trainable_params': [65784578],\n",
       "   'n_trainable_params_perc': [0.2241497999728751]},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': [184320],\n",
       "   'total_trainable_params': [65784578],\n",
       "   'n_trainable_params_perc': [0.2801872499660939]},\n",
       "  'P_TUNING': {'n_trainable_params': [229376],\n",
       "   'total_trainable_params': [65784578],\n",
       "   'n_trainable_params_perc': [0.34867746662447235]}},\n",
       " 'tiny': {'PROMPT_TUNING': {'n_trainable_params': [4372],\n",
       "   'total_trainable_params': [13874762],\n",
       "   'n_trainable_params_perc': [0.03151045041349178]},\n",
       "  'LORA': {'n_trainable_params': [39936],\n",
       "   'total_trainable_params': [13874762],\n",
       "   'n_trainable_params_perc': [0.2878319642527922]},\n",
       "  'PREFIX_TUNING': {'n_trainable_params': [49920],\n",
       "   'total_trainable_params': [13874762],\n",
       "   'n_trainable_params_perc': [0.3597899553159903]},\n",
       "  'P_TUNING': {'n_trainable_params': [103064],\n",
       "   'total_trainable_params': [13874762],\n",
       "   'n_trainable_params_perc': [0.7428163452461383]}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trainable_params.yaml\", 'r') as f:\n",
    "    reloaded = yaml.load(f, yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LORA': {'n_trainable_params': [147456],\n",
       "  'n_trainable_params_perc': [0.2241497999728751],\n",
       "  'total_trainable_params': [65784578]},\n",
       " 'PREFIX_TUNING': {'n_trainable_params': [184320],\n",
       "  'n_trainable_params_perc': [0.2801872499660939],\n",
       "  'total_trainable_params': [65784578]},\n",
       " 'PROMPT_TUNING': {'n_trainable_params': [10756],\n",
       "  'n_trainable_params_perc': [0.016350336700495363],\n",
       "  'total_trainable_params': [65784578]},\n",
       " 'P_TUNING': {'n_trainable_params': [229376],\n",
       "  'n_trainable_params_perc': [0.34867746662447235],\n",
       "  'total_trainable_params': [65784578]}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded[\"distil\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
