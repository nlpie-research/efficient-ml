{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "# import dataloader from torch\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find the model with the latest date in this folder\n",
    "\n",
    "def extract_datetime(x):\n",
    "    ''' \n",
    "    Extract the datetime from a string\n",
    "\n",
    "    Args:\n",
    "    x (str): string to extract datetime from\n",
    "    \n",
    "    Returns:\n",
    "    datetime: datetime object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dt = x.split('/')[-1]\n",
    "    # print(dt)\n",
    "    dt = datetime.strptime(dt, '%d-%m-%Y--%H-%M')\n",
    "    return dt\n",
    "\n",
    "def get_latest_model(model_dir):\n",
    "    \n",
    "    ''' \n",
    "    Get the latest model in a directory\n",
    "    \n",
    "    Args:\n",
    "    model_dir (str): directory to search for the latest model\n",
    "    \n",
    "    Returns:\n",
    "    str: path to the latest model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # first list the full paths of all subdirectories\n",
    "    subdirs = [os.path.join(model_dir, o) for o in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir,o))]\n",
    "    \n",
    "    # now loop over the subdirs and extract the datetime from the path and sort by datetime\n",
    "    subdirs = sorted(subdirs, key=extract_datetime)\n",
    "    \n",
    "    # latest model is the last one\n",
    "    latest_subdir = subdirs[-1]\n",
    "    \n",
    "    # now we want to find the checkpoint folder in this directory that has the highest tail number split on '-'\n",
    "    # lets get the paths inside the latest_subdir\n",
    "    subsubdirs = [os.path.join(latest_subdir, o) for o in os.listdir(latest_subdir) if os.path.isdir(os.path.join(latest_subdir,o))]\n",
    "    \n",
    "    # now loop over the subdirs and extract the tail number from the path and sort by tail number\n",
    "    subsubdirs = sorted(subsubdirs, key=lambda x: int(x.split('-')[-1]))\n",
    "    \n",
    "    # latest model is the last one\n",
    "    latest_model = subsubdirs[-1]\n",
    "    return latest_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory for a task and model \n",
    "\n",
    "task = \"mimic-mp\"\n",
    "peft_type = \"LORA\" # | Full\n",
    "model_name = \"bio-mobilebert\"\n",
    "\n",
    "model_dir = f\"/mnt/sdh/effecient_ml/ckpts/mimic-mp/full/{model_name}/{peft_type}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_dir = get_latest_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/sdh/effecient_ml/ckpts/mimic-mp/full/bio-mobilebert/LORA/01-12-2023--13-10/checkpoint-5310'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98e8c5ea8924c6bad3b225dd04abd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ddc18445c64c838e054abd6b75321a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/910k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NTaylor/bio-mobilebert-mimic-mp-lora/commit/0d5e0fcb1b33fdfb94f005a5affbcfab47f49a96', commit_message='Upload model', commit_description='', oid='0d5e0fcb1b33fdfb94f005a5affbcfab47f49a96', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets test pushing to hub\n",
    "reloaded_model.push_to_hub(\"NTaylor/bio-mobilebert-mimic-mp-lora\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning** \n",
    "\n",
    "The AutoPeftModel... seems to actually work based on the eval performance. \n",
    "The warning above is related to the base model being loaded and I guess the library doesn't notice that the adapter weights include the classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0136,  0.0475,  0.0532,  ...,  0.0615,  0.0177,  0.0178],\n",
       "        [ 0.0028,  0.0364, -0.0520,  ..., -0.0609, -0.0284, -0.0447]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model.classifier.modules_to_save.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(full_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure the autopeft model is working well with all models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try more manual reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "config = PeftConfig.from_pretrained(full_model_dir)\n",
    "# load base model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='nlpie/bio-mobilebert', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules={'value', 'query', 'key'}, lora_alpha=8, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = PeftModel.from_pretrained(model, full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MobileBertForSequenceClassification(\n",
       "      (mobilebert): MobileBertModel(\n",
       "        (embeddings): MobileBertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 512)\n",
       "          (token_type_embeddings): Embedding(2, 512)\n",
       "          (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (LayerNorm): NoNorm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): MobileBertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x MobileBertLayer(\n",
       "              (attention): MobileBertAttention(\n",
       "                (self): MobileBertSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=512, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): MobileBertSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (intermediate): MobileBertIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): ReLU()\n",
       "              )\n",
       "              (output): MobileBertOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (LayerNorm): NoNorm()\n",
       "                (bottleneck): OutputBottleneck(\n",
       "                  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (bottleneck): Bottleneck(\n",
       "                (input): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "                (attention): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (ffn): ModuleList(\n",
       "                (0-2): 3 x FFNLayer(\n",
       "                  (intermediate): MobileBertIntermediate(\n",
       "                    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                    (intermediate_act_fn): ReLU()\n",
       "                  )\n",
       "                  (output): FFNOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                    (LayerNorm): NoNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): MobileBertPooler()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=512, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=512, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge and unload\n",
    "# reloaded_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('../datasets.yaml', 'r') as f:\n",
    "    datasets = yaml.load(f, yaml.FullLoader)\n",
    "\n",
    "try:\n",
    "    dataset_info = datasets[task]\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Task name {task} not in datasets.yaml. Available tasks are: {list(datasets.keys())}\")\n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_data_dir': '/mnt/sdd/efficient_ml_data/datasets/mimic3-clinical-outcomes/mp',\n",
       " 'eval_data_dir': '/mnt/sdd/efficient_ml_data/datasets/mimic3-clinical-outcomes/mp',\n",
       " 'data_dir': '',\n",
       " 'training_file': 'train.csv',\n",
       " 'validation_file': 'valid.csv',\n",
       " 'test_file': 'test.csv',\n",
       " 'task_type': 'SEQ_CLS',\n",
       " 'label_name': 'hospital_expire_flag',\n",
       " 'text_column': 'text',\n",
       " 'remove_columns': ['text']}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"csv\", \n",
    "                        data_files = {\"train\":f\"{dataset_info['training_data_dir']}/{dataset_info['training_file']}\",\n",
    "                                    \"validation\":f\"{dataset_info['eval_data_dir']}/{dataset_info['validation_file']}\",\n",
    "                                    \"test\":f\"{dataset_info['eval_data_dir']}/{dataset_info['validation_file']}\",\n",
    "                                    },\n",
    "                    cache_dir = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of various datasets and their sentence keys\n",
    "task_to_keys ={\n",
    "                \"cola\": (\"sentence\", None),\n",
    "                \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "                \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "                \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "                \"qnli\": (\"question\", \"sentence\"),\n",
    "                \"qqp\": (\"question1\", \"question2\"),\n",
    "                \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "                \"sst2\": (\"sentence\", None),\n",
    "                \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "                \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "                \"mimic-note-category\": (\"TEXT\", None),\n",
    "                \"icd9-triage\":(\"text\", None),\n",
    "                \"icd9-triage-no-category-in-text\":(\"text\", None),\n",
    "                \"ICD9-Triage\":(\"text\", None),\n",
    "                \"mednli\":(\"sentence1\", \"sentence2\"),\n",
    "                \"mimic-mp\":(dataset_info[\"text_column\"], None),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of labels\n",
    "num_labels = len(np.unique(datasets[\"train\"][dataset_info[\"label_name\"]]))\n",
    "\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464277b48e7444adbff13196baa41271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a95302d3ce4eba98c14914f180a846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638cc10fab7047818630c01567044f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = full_model_dir\n",
    "batch_size = 16\n",
    "if any(k in full_model_dir for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "# own\n",
    "def tokenize_function(examples):\n",
    "    # max_length is important when using prompt tuning  or prefix tuning or p tuning as virtual tokens are added - which can overshoot the max length in pefts current form\n",
    "    # for now set to 480 and see how it goes\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, max_length = 480)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=480)\n",
    "\n",
    "# own\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_info[\"remove_columns\"]\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "if \"labels\" not in tokenized_datasets[\"train\"].features:\n",
    "        tokenized_datasets = tokenized_datasets.rename_column(dataset_info[\"label_name\"], \"labels\")\n",
    "\n",
    "# Instantiate dataloaders.\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor([176763, 173211, 116333, 161102, 116799, 162982, 114396, 143396, 108327,\n",
      "        143022, 130766, 140160, 140485, 166483, 184549, 104154]), 'labels': tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]), 'input_ids': tensor([[  101,  2708, 12087,  ...,  2575,  2232,   102],\n",
      "        [  101,  2708, 12087,  ...,  1008,  2902,   102],\n",
      "        [  101,  2708, 12087,  ...,  1011, 16021,   102],\n",
      "        ...,\n",
      "        [  101,  2708, 12087,  ...,  1007,  1012,   102],\n",
      "        [  101,  2708, 12087,  ...,     0,     0,     0],\n",
      "        [  101,  2708, 12087,  ...,  1999,  1031,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# check eval dataloader\n",
    "for batch in eval_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_score, recall_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, pred_scores, labels):\n",
    "    \n",
    "    # use from evaluate for now\n",
    "    precision_score = evaluate.load(\"precision\")\n",
    "    recall_score = evaluate.load(\"recall\")\n",
    "    accuracy_score = evaluate.load(\"accuracy\")\n",
    "    f1_score = evaluate.load(\"f1\")    \n",
    "          \n",
    "    print(f\"Labels are: {labels}\\n\")\n",
    "    print(f\"Preds are: {predictions}\")\n",
    "    precision = precision_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"precision\"]\n",
    "    recall = recall_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"recall\"]\n",
    "    accuracy = accuracy_score.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"f1\"]\n",
    "    f1_weighted = f1_score.compute(predictions=predictions, references=labels, average = \"weighted\")[\"f1\"]\n",
    "    # roc_auc has slightly different format - needs the probs/scores rather than predicted labels\n",
    "    # change roc based on number of labels\n",
    "    if len(np.unique(labels)) == 2:   \n",
    "\n",
    "        roc_auc_score = evaluate.load(\"roc_auc\", \"binary\")\n",
    "        roc_auc = roc_auc_score.compute(references=labels,\n",
    "                                        # just take the probabilties of the positive class\n",
    "                                        prediction_scores = pred_scores[:,1]                                         \n",
    "                                        )['roc_auc']\n",
    "    else:\n",
    "        roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "        roc_auc = roc_auc_score.compute(references=labels,\n",
    "                                        prediction_scores = pred_scores,\n",
    "                                        multi_class = 'ovr', \n",
    "                                        average = \"macro\")['roc_auc']        \n",
    "   \n",
    "    return {\"precision\": precision, \n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_macro\":f1_macro,\n",
    "            \"f1_weighted\":f1_weighted,\n",
    "            \"roc_auc_macro\":roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:26<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds_raw shape is: (4908, 2)\n",
      "all_preds shape is: (4908,) \n",
      "\n",
      " [0 0 0 ... 0 0 0]\n",
      "all_labels shape is: (4908,) \n",
      "\n",
      " [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# send model to cuda\n",
    "model.cuda()\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_preds_raw = []\n",
    "all_labels = []\n",
    "for batch in tqdm(eval_dataloader):               \n",
    "    with torch.no_grad():\n",
    "        # send batch to cuda\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        outputs = reloaded_model(input_ids = batch[\"input_ids\"], \n",
    "                                 attention_mask = batch[\"attention_mask\"],\n",
    "                                 token_type_ids = batch[\"token_type_ids\"])\n",
    "\n",
    "    # apply softmax to the logits of the output - using the softmax function\n",
    "    preds_raw = outputs.logits.softmax(dim=-1).cpu()           \n",
    "\n",
    "    \n",
    "    # get argmax of preds raw\n",
    "    preds = np.argmax(preds_raw, axis = -1)             \n",
    "    \n",
    "    all_preds_raw.extend(list(preds_raw))\n",
    "    all_preds.extend(list(preds))\n",
    "    all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n",
    "\n",
    "\n",
    "\n",
    "all_preds_raw = np.stack(all_preds_raw)\n",
    "all_preds = np.stack(all_preds)\n",
    "all_labels = np.stack(all_labels)\n",
    "\n",
    "print(f\"all_preds_raw shape is: {all_preds_raw.shape}\")\n",
    "print(f\"all_preds shape is: {all_preds.shape} \\n\\n {all_preds}\")\n",
    "print(f\"all_labels shape is: {all_labels.shape} \\n\\n {all_labels}\")\n",
    "# print(f\"all_embeddings shape is: {all_embeddings.shape} \\n\\n {all_embeddings}\")\n",
    "# metrics = all_metrics(yhat=all_preds, y=all_labels, yhat_raw=all_preds_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are: [0 0 1 ... 0 0 0]\n",
      "\n",
      "Preds are: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# use compute metrics with args: predictions, pred_scores, labels\n",
    "metrics = compute_metrics(all_preds, all_preds_raw, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7036768485908739,\n",
       " 'recall': 0.5350194809192531,\n",
       " 'accuracy': 0.8946617766911166,\n",
       " 'f1_macro': 0.5404014104010101,\n",
       " 'f1_weighted': 0.8587342343219413,\n",
       " 'roc_auc_macro': 0.7897045760371499}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
