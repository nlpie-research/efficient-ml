{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "# import dataloader from torch\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set cuda visible to 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152f492faff44223b74066e564ee9d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, create_repo\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find the model with the latest date in this folder\n",
    "\n",
    "def extract_datetime(x):\n",
    "    ''' \n",
    "    Extract the datetime from a string\n",
    "\n",
    "    Args:\n",
    "    x (str): string to extract datetime from\n",
    "    \n",
    "    Returns:\n",
    "    datetime: datetime object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dt = x.split('/')[-1]\n",
    "    # print(dt)\n",
    "    dt = datetime.strptime(dt, '%d-%m-%Y--%H-%M')\n",
    "    return dt\n",
    "\n",
    "def get_latest_model(model_dir):\n",
    "    \n",
    "    ''' \n",
    "    Get the latest model in a directory\n",
    "    \n",
    "    Args:\n",
    "    model_dir (str): directory to search for the latest model\n",
    "    \n",
    "    Returns:\n",
    "    str: path to the latest model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # first list the full paths of all subdirectories\n",
    "    subdirs = [os.path.join(model_dir, o) for o in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir,o))]\n",
    "    \n",
    "    # check we have any subdirs\n",
    "    if len(subdirs) == 0:\n",
    "        raise ValueError('No subdirectories found in model_dir')\n",
    "    \n",
    "    \n",
    "    # now loop over the subdirs and extract the datetime from the path and sort by datetime\n",
    "    subdirs = sorted(subdirs, key=extract_datetime)\n",
    "    \n",
    "    # latest model is the last one\n",
    "    latest_subdir = subdirs[-1]\n",
    "    \n",
    "    print(f'Latest subdir: {latest_subdir}')\n",
    "    \n",
    "    # now we want to find the checkpoint folder in this directory that has the highest tail number split on '-'\n",
    "    # lets get the paths inside the latest_subdir\n",
    "    \n",
    "    # if we have no checkpoint folder - then we want to just pull the latest model from the latest_subdir\n",
    "    if any(\"checkpoint\" in s for s in latest_subdir):\n",
    "    \n",
    "        subsubdirs = [os.path.join(latest_subdir, o) for o in os.listdir(latest_subdir) if os.path.isdir(os.path.join(latest_subdir,o))]\n",
    "        \n",
    "        print(f\"subsubdirs: {subsubdirs}\")\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # now loop over the subdirs and extract the tail number from the path and sort by tail number\n",
    "        subsubdirs = sorted(subsubdirs, key=lambda x: int(x.split('-')[-1]))\n",
    "        \n",
    "        # latest model is the last one\n",
    "        latest_model = subsubdirs[-1]\n",
    "        return latest_model\n",
    "    \n",
    "    else:\n",
    "        # check if the latest subdir is empty\n",
    "        if len(os.listdir(latest_subdir)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return latest_subdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory for a task and model \n",
    "\n",
    "task = \"mimic-mp\"\n",
    "peft_type = \"LORA\" # | Full\n",
    "model_name = \"Llama-2-7b-hf\" # Llama-2-7b | bio-mobilebert\n",
    "\n",
    "model_dir = f\"/mnt/sdh/effecient_ml/ckpts/{task}/full/{model_name}/{peft_type}/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest subdir: /mnt/sdh/effecient_ml/ckpts/mimic-mp/full/Llama-2-7b-hf/LORA/29-09-2023--12-22\n"
     ]
    }
   ],
   "source": [
    "full_model_dir = get_latest_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/sdh/effecient_ml/ckpts/mimic-mp/full/Llama-2-7b-hf/LORA/29-09-2023--12-22'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '{full_model_dir}': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls {full_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "The current llama adapter weights we have do not seem to work properly and lead to nan outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# # try just llama for sanity check\n",
    "# full_model_dir = \"meta-llama/Llama-2-7b-hf\"\n",
    "# reloaded_model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set recent llama dir\n",
    "full_model_dir = \"/mnt/sdh/effecient_ml/fewshot_budget/ckpts/mimic-mp/fewshot_4096/Llama-2-7b-hf/LORA/04-03-2024--18-57/checkpoint-2380/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload from hf hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "full_model_dir = \"NTaylor/bio-mobilebert-mimic-mp-lora\"\n",
    "\n",
    "# load using AutoPeftModelForSequenceClassification\n",
    "reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a742ce4dd7a49998a2c60e37be1a702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# if Llama load in bfloat 16\n",
    "if \"Llama\" in model_name:\n",
    "    reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(full_model_dir,     # torch.bfloat16 throws errors later                       \n",
    "                                                                 torch_dtype=torch.bfloat16)\n",
    "else:\n",
    "    reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Llama-2-7b-hf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model.config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/sdh/effecient_ml/fewshot_budget/ckpts/mimic-mp/fewshot_4096/Llama-2-7b-hf/LORA/01-03-2024--13-29/checkpoint-2380/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dtype of model weights\n",
    "# for name, param in reloaded_model.named_parameters():\n",
    "#     print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't have a tokenizer saved for NTaylor/bio-mobilebert-mimic-mp-lora\n"
     ]
    }
   ],
   "source": [
    "# some may not have a tokenizer saved - I think the library changed at some point and now saves everything? \n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_model_dir)\n",
    "\n",
    "except:\n",
    "    print(f\"Didn't have a tokenizer saved for {full_model_dir}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(reloaded_model.config._name_or_path)\n",
    "    # and now save to the same repo as the full model\n",
    "    tokenizer.save_pretrained(full_model_dir)\n",
    "    \n",
    "\n",
    "    \n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    print(f\"Adding pad token manually! Setting pad token to eos token: {tokenizer.eos_token_id}\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id   \n",
    "    \n",
    "# set config token id\n",
    "if \"Llama\" in model_name:\n",
    "    reloaded_model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileBertTokenizerFast(name_or_path='nlpie/bio-mobilebert', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2-7b-hf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d22ae3131841eb817648247e236567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NTaylor/Llama-2-7b-hf-mimic-mp-lora/commit/bd5f6176abf109e50263d801e2822977b9552896', commit_message='Upload model', commit_description='', oid='bd5f6176abf109e50263d801e2822977b9552896', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets test pushing to hub\n",
    "reloaded_model.push_to_hub(f\"NTaylor/{model_name}-mimic-mp-lora\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning** \n",
    "\n",
    "The AutoPeftModel... seems to actually work based on the eval performance. \n",
    "The warning above is related to the base model being loaded and I guess the library doesn't notice that the adapter weights include the classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0126, -0.0219,  0.0136,  ...,  0.0066, -0.0023, -0.0223],\n",
       "        [ 0.0250, -0.0125,  0.0127,  ...,  0.0085, -0.0003, -0.0071],\n",
       "        [ 0.0091,  0.0018, -0.0088,  ..., -0.0028,  0.0084, -0.0276],\n",
       "        ...,\n",
       "        [ 0.0237, -0.0330, -0.0233,  ..., -0.0121, -0.0133, -0.0488],\n",
       "        [ 0.0159,  0.0172,  0.0170,  ..., -0.0229, -0.0216,  0.0198],\n",
       "        [-0.0771, -0.0320,  0.0109,  ..., -0.0105, -0.0004,  0.0040]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reloaded_model.classifier.modules_to_save.default.weight\n",
    "\n",
    "reloaded_model.base_model.model.model.layers[31].self_attn.k_proj.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure the autopeft model is working well with all models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to loop over many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_models_to_hub(model_name, task, peft_type, use_auth_token=True):\n",
    "    \n",
    "    # set the model dir dynamically base on the task and model name\n",
    "    model_dir = f\"/mnt/sdh/effecient_ml/ckpts/{task}/full/{model_name}/{peft_type}/\"\n",
    "    \n",
    "    # get the latest model\n",
    "    full_model_dir = get_latest_model(model_dir)\n",
    "    \n",
    "    # check if full_model_dir is None\n",
    "    if full_model_dir is None:\n",
    "        print(f\"No models found in {model_dir}\")\n",
    "    else:\n",
    "        print(f\"Pushing {full_model_dir} to hub\")\n",
    "        # reload using PEFT\n",
    "        reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(full_model_dir)\n",
    "        \n",
    "        # some may not have a tokenizer saved - I think the library changed at some point and now saves everything? \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(full_model_dir)\n",
    "        except:\n",
    "            print(f\"Didn't have a tokenizer saved for {full_model_dir}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(reloaded_model.config._name_or_path)\n",
    "            # and now save to the same repo as the full model\n",
    "            tokenizer.save_pretrained(full_model_dir)\n",
    "        # push to hub\n",
    "        reloaded_model.push_to_hub(f\"NTaylor/{model_name}-{task}-{peft_type}\", use_auth_token=use_auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Pushing Llama-2-7b-hf ##############\n",
      "Latest subdir: /mnt/sdh/effecient_ml/ckpts/mimic-mp/full/Llama-2-7b-hf/LORA/29-09-2023--12-22\n",
      "Pushing /mnt/sdh/effecient_ml/ckpts/mimic-mp/full/Llama-2-7b-hf/LORA/29-09-2023--12-22 to hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def648f216ed4511856bbd849203931e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb3c76b1a9b44f4a5c72aa94691120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# models to push\n",
    "models_to_push = [\n",
    "                    # \"clinical-mobilebert\",                    \n",
    "                    # \"bio-mobilebert\",\n",
    "                    # \"clinical-distilbert\",\n",
    "                    # \"distil-biobert\",\n",
    "                    # \"tiny-biobert\",\n",
    "                    # \"tiny-clinicalbert\",\n",
    "                    \"Llama-2-7b-hf\"\n",
    "                        ]\n",
    "\n",
    "task = \"mimic-mp\"\n",
    "# pass through funciton\n",
    "for model_name in tqdm(models_to_push):\n",
    "    print(f\"############## Pushing {model_name} ##############\")\n",
    "    push_models_to_hub(model_name, task, \"LORA\", use_auth_token=True)\n",
    "    # push_models_to_hub(model_name, task, \"Full\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try more manual reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "config = PeftConfig.from_pretrained(full_model_dir)\n",
    "# load base model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='nlpie/bio-mobilebert', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules={'value', 'query', 'key'}, lora_alpha=8, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = PeftModel.from_pretrained(model, full_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MobileBertForSequenceClassification(\n",
       "      (mobilebert): MobileBertModel(\n",
       "        (embeddings): MobileBertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 512)\n",
       "          (token_type_embeddings): Embedding(2, 512)\n",
       "          (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (LayerNorm): NoNorm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): MobileBertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x MobileBertLayer(\n",
       "              (attention): MobileBertAttention(\n",
       "                (self): MobileBertSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=512, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): MobileBertSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (intermediate): MobileBertIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): ReLU()\n",
       "              )\n",
       "              (output): MobileBertOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (LayerNorm): NoNorm()\n",
       "                (bottleneck): OutputBottleneck(\n",
       "                  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (bottleneck): Bottleneck(\n",
       "                (input): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "                (attention): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (ffn): ModuleList(\n",
       "                (0-2): 3 x FFNLayer(\n",
       "                  (intermediate): MobileBertIntermediate(\n",
       "                    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                    (intermediate_act_fn): ReLU()\n",
       "                  )\n",
       "                  (output): FFNOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                    (LayerNorm): NoNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): MobileBertPooler()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=512, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=512, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge and unload\n",
    "# reloaded_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('../datasets.yaml', 'r') as f:\n",
    "    datasets = yaml.load(f, yaml.FullLoader)\n",
    "\n",
    "try:\n",
    "    dataset_info = datasets[task]\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Task name {task} not in datasets.yaml. Available tasks are: {list(datasets.keys())}\")\n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_data_dir': '/mnt/sdd/efficient_ml_data/datasets/mimic3-clinical-outcomes/mp',\n",
       " 'eval_data_dir': '/mnt/sdd/efficient_ml_data/datasets/mimic3-clinical-outcomes/mp',\n",
       " 'data_dir': '',\n",
       " 'training_file': 'train.csv',\n",
       " 'validation_file': 'valid.csv',\n",
       " 'test_file': 'test.csv',\n",
       " 'task_type': 'SEQ_CLS',\n",
       " 'label_name': 'hospital_expire_flag',\n",
       " 'text_column': 'text',\n",
       " 'remove_columns': ['text']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"csv\", \n",
    "                        data_files = {\"train\":f\"{dataset_info['training_data_dir']}/{dataset_info['training_file']}\",\n",
    "                                    \"validation\":f\"{dataset_info['eval_data_dir']}/{dataset_info['validation_file']}\",\n",
    "                                    \"test\":f\"{dataset_info['eval_data_dir']}/{dataset_info['validation_file']}\",\n",
    "                                    },\n",
    "                    cache_dir = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of various datasets and their sentence keys\n",
    "task_to_keys ={\n",
    "                \"cola\": (\"sentence\", None),\n",
    "                \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "                \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "                \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "                \"qnli\": (\"question\", \"sentence\"),\n",
    "                \"qqp\": (\"question1\", \"question2\"),\n",
    "                \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "                \"sst2\": (\"sentence\", None),\n",
    "                \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "                \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "                \"mimic-note-category\": (\"TEXT\", None),\n",
    "                \"icd9-triage\":(\"text\", None),\n",
    "                \"icd9-triage-no-category-in-text\":(\"text\", None),\n",
    "                \"ICD9-Triage\":(\"text\", None),\n",
    "                \"mednli\":(\"sentence1\", \"sentence2\"),\n",
    "                \"mimic-mp\":(dataset_info[\"text_column\"], None),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of labels\n",
    "num_labels = len(np.unique(datasets[\"train\"][dataset_info[\"label_name\"]]))\n",
    "\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de8b54c8ed44e48a5b2247f4f263cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = full_model_dir\n",
    "batch_size = 2 # 2 for llama\n",
    "if any(k in full_model_dir for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "# own\n",
    "def tokenize_function(examples):\n",
    "    # max_length is important when using prompt tuning  or prefix tuning or p tuning as virtual tokens are added - which can overshoot the max length in pefts current form\n",
    "    # for now set to 480 and see how it goes\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, max_length = 480)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=480)\n",
    "\n",
    "# own\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_info[\"remove_columns\"]\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "if \"labels\" not in tokenized_datasets[\"train\"].features:\n",
    "        tokenized_datasets = tokenized_datasets.rename_column(dataset_info[\"label_name\"], \"labels\")\n",
    "\n",
    "# Instantiate dataloaders.\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MobileBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# check eval dataloader\n",
    "for batch in eval_dataloader:\n",
    "    # print(batch)\n",
    "    # pass to model \n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    outputs = reloaded_model(input_ids = batch[\"input_ids\"], \n",
    "                                 attention_mask = batch[\"attention_mask\"],\n",
    "                                #  token_type_ids = batch[\"token_type_ids\"]\n",
    "                                 )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0571, -0.2056],\n",
       "        [ 2.2753, -1.6154]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_score, recall_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, pred_scores, labels):\n",
    "    \n",
    "    # use from evaluate for now\n",
    "    precision_score = evaluate.load(\"precision\")\n",
    "    recall_score = evaluate.load(\"recall\")\n",
    "    accuracy_score = evaluate.load(\"accuracy\")\n",
    "    f1_score = evaluate.load(\"f1\")    \n",
    "          \n",
    "    print(f\"Labels are: {labels}\\n\")\n",
    "    print(f\"Preds are: {predictions}\")\n",
    "    precision = precision_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"precision\"]\n",
    "    recall = recall_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"recall\"]\n",
    "    accuracy = accuracy_score.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_score.compute(predictions=predictions, references=labels, average = \"macro\")[\"f1\"]\n",
    "    f1_weighted = f1_score.compute(predictions=predictions, references=labels, average = \"weighted\")[\"f1\"]\n",
    "    # roc_auc has slightly different format - needs the probs/scores rather than predicted labels\n",
    "    # change roc based on number of labels\n",
    "    if len(np.unique(labels)) == 2:   \n",
    "\n",
    "        roc_auc_score = evaluate.load(\"roc_auc\", \"binary\")\n",
    "        roc_auc = roc_auc_score.compute(references=labels,\n",
    "                                        # just take the probabilties of the positive class\n",
    "                                        prediction_scores = pred_scores[:,1]                                         \n",
    "                                        )['roc_auc']\n",
    "    else:\n",
    "        roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "        roc_auc = roc_auc_score.compute(references=labels,\n",
    "                                        prediction_scores = pred_scores,\n",
    "                                        multi_class = 'ovr', \n",
    "                                        average = \"macro\")['roc_auc']        \n",
    "   \n",
    "    return {\"precision\": precision, \n",
    "            \"recall\": recall,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_macro\":f1_macro,\n",
    "            \"f1_weighted\":f1_weighted,\n",
    "            \"roc_auc_macro\":roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2454/2454 [01:41<00:00, 24.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds_raw shape is: (4908, 2)\n",
      "all_preds shape is: (4908,) \n",
      "\n",
      " [0 0 0 ... 0 0 0]\n",
      "all_labels shape is: (4908,) \n",
      "\n",
      " [0 0 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# send model to cuda\n",
    "reloaded_model.cuda()\n",
    "reloaded_model.eval()\n",
    "all_preds = []\n",
    "all_preds_raw = []\n",
    "all_labels = []\n",
    "for batch in tqdm(eval_dataloader):               \n",
    "    with torch.no_grad():\n",
    "        # send batch to cuda\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        outputs = reloaded_model(input_ids = batch[\"input_ids\"], \n",
    "                                 attention_mask = batch[\"attention_mask\"],\n",
    "                                #  token_type_ids = batch[\"token_type_ids\"]\n",
    "                                 )\n",
    "\n",
    "    # apply softmax to the logits of the output - using the softmax function\n",
    "    preds_raw = outputs.logits.softmax(dim=-1).cpu().float()           \n",
    "\n",
    "    \n",
    "    # get argmax of preds raw\n",
    "    preds = torch.argmax(preds_raw, axis = -1)             \n",
    "    \n",
    "    all_preds_raw.extend(list(preds_raw))\n",
    "    all_preds.extend(list(preds))\n",
    "    all_labels.extend(list(batch[\"labels\"].cpu().numpy()))\n",
    "\n",
    "\n",
    "\n",
    "all_preds_raw = np.stack(all_preds_raw)\n",
    "all_preds = np.stack(all_preds)\n",
    "all_labels = np.stack(all_labels)\n",
    "\n",
    "print(f\"all_preds_raw shape is: {all_preds_raw.shape}\")\n",
    "print(f\"all_preds shape is: {all_preds.shape} \\n\\n {all_preds}\")\n",
    "print(f\"all_labels shape is: {all_labels.shape} \\n\\n {all_labels}\")\n",
    "# print(f\"all_embeddings shape is: {all_embeddings.shape} \\n\\n {all_embeddings}\")\n",
    "# metrics = all_metrics(yhat=all_preds, y=all_labels, yhat_raw=all_preds_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test torch argmax feature\n",
    "a = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]])\n",
    "torch.argmax(a, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tensor of bfloat 16\n",
    "a = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]], dtype = torch.bfloat16)\n",
    "\n",
    "# convert to float16\n",
    "b = a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test numpy argmax feature\n",
    "b = np.array([[0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]])\n",
    "np.argmax(b, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are: [0 0 1 ... 0 0 0]\n",
      "\n",
      "Preds are: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# use compute metrics with args: predictions, pred_scores, labels\n",
    "metrics = compute_metrics(all_preds, all_preds_raw, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4827,   81]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get value counts of all preds\n",
    "np.unique(all_preds, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7036768485908739,\n",
       " 'recall': 0.5350194809192531,\n",
       " 'accuracy': 0.8946617766911166,\n",
       " 'f1_macro': 0.5404014104010101,\n",
       " 'f1_weighted': 0.8587342343219413,\n",
       " 'roc_auc_macro': 0.7897098530355934}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test reloading from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at nlpie/bio-mobilebert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# lora_id = \"NTaylor/Llama-2-7b-hf-mimic-mp-lora\" \n",
    "lora_id = \"NTaylor/bio-mobilebert-mimic-mp-lora\"\n",
    "\n",
    "# load using AutoPeftModelForSequenceClassification\n",
    "reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(lora_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MobileBertForSequenceClassification(\n",
       "      (mobilebert): MobileBertModel(\n",
       "        (embeddings): MobileBertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 512)\n",
       "          (token_type_embeddings): Embedding(2, 512)\n",
       "          (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (LayerNorm): NoNorm()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): MobileBertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x MobileBertLayer(\n",
       "              (attention): MobileBertAttention(\n",
       "                (self): MobileBertSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=512, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): MobileBertSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (intermediate): MobileBertIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): ReLU()\n",
       "              )\n",
       "              (output): MobileBertOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (LayerNorm): NoNorm()\n",
       "                (bottleneck): OutputBottleneck(\n",
       "                  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (bottleneck): Bottleneck(\n",
       "                (input): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "                (attention): BottleneckLayer(\n",
       "                  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (LayerNorm): NoNorm()\n",
       "                )\n",
       "              )\n",
       "              (ffn): ModuleList(\n",
       "                (0-2): 3 x FFNLayer(\n",
       "                  (intermediate): MobileBertIntermediate(\n",
       "                    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                    (intermediate_act_fn): ReLU()\n",
       "                  )\n",
       "                  (output): FFNOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                    (LayerNorm): NoNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): MobileBertPooler()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=512, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=512, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpie/bio-mobilebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# long version worked for llama\n",
    "# text = \"82 year old patient initially presented with severe chest pain. They have a history of heart attacks, and there has been a struggle to bring the heart into a normal rythym .\"\n",
    "text = \"They are likely to pass away\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = reloaded_model(**inputs)\n",
    "# extract prediction from outputs based on argmax of logits\n",
    "pred = torch.argmax(outputs.logits, axis = -1)\n",
    "# binary classification: 1 is positive for mortality \n",
    "print(f\"Prediction is: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4350, -1.5346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
