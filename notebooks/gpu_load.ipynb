{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#set visible cuda devices\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForTokenClassification, \n",
    "                          AutoModelForCausalLM,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          AutoModel,\n",
    "                          AutoTokenizer,\n",
    ")\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from data_utils.model_utils import count_trainable_parameters, freeze_model, unfreeze_model\n",
    "from thop import profile # for flops calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get flops n params\n",
    "def get_flops_and_params(model_name):\n",
    "    \n",
    "    if \"llama\" in model_name:\n",
    "        print(\"Got llama model, loading in half precision\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                        torch_dtype = torch.bfloat16,\n",
    "                                        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        \n",
    "    input_ids = torch.tensor([[101, 2023, 2003, 1037, 2047, 2814, 1012, 102]])\n",
    "    flops, params = profile(model, inputs=(input_ids,))\n",
    "    # convert flops to scientific notation\n",
    "    flops = \"{:.2e}\".format(flops)\n",
    "    return flops, params\n",
    "\n",
    "# turn above into a function that accepts multiple models and returns the gpu memory needed\n",
    "# it should take in a list of model names and return a dictionary of model names and gpu memory needed\n",
    "\n",
    "def get_gpu_memory_needed(model_names):\n",
    "    device = torch.device('cuda:0') \n",
    "    gpu_memory_needed = {}\n",
    "    for model_name in tqdm(model_names):\n",
    "        \n",
    "        if \"llama\" in model_name:\n",
    "            print(\"Got llama model, loading in half precision\")\n",
    "            model = AutoModel.from_pretrained(model_name,\n",
    "                                            torch_dtype = torch.bfloat16,\n",
    "                                            device_map = \"auto\")\n",
    "        else:\n",
    "            model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        gpu_memory_needed[model_name] = torch.cuda.memory_allocated(device.index)/1024**3\n",
    "        \n",
    "        # get flops and num params\n",
    "        flops, n_params = get_flops_and_params(model_name)\n",
    "    return gpu_memory_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got llama model, loading in half precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdc/niallt/venvs/39nlp/lib/python3.9/site-packages/accelerate/utils/imports.py:245: UserWarning: Intel Extension for PyTorch 1.12 needs to work with PyTorch 1.12.*, but PyTorch 2.1.2 is found. Please switch to the matching version and run again.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7330b4c06984690b31e5fc1e52c4620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    # \"nlpie/bio-mobilebert\",\n",
    "    #                 \"nlpie/tiny-biobert\",\n",
    "    #                 \"roberta-base\",\n",
    "    #                 \"nlpie/distil-biobert\",\n",
    "    #                 \"dmis-lab/biobert-v1.1\",\n",
    "                     \"meta-llama/Llama-2-7b-hf\"\n",
    "                     ]\n",
    "\n",
    "gpu_memory_needed = get_gpu_memory_needed(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta-llama/Llama-2-7b-hf': 12.369651794433594}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_memory_needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "with open('../gpu_memory_needed.json', 'w') as fp:\n",
    "    json.dump(gpu_memory_needed, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd4adf50c564883a0e86fc6900198a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Create a sample input\n",
    "# input_ids = torch.tensor([[101, 2023, 2003, 1037, 2047, 2814, 1012, 102]])\n",
    "\n",
    "# # Use the thop library to profile the model\n",
    "# flops, params = profile(model, inputs=(input_ids,))\n",
    "\n",
    "# # Print the estimated FLOPs and number of parameters\n",
    "# print(f\"FLOPs: {flops}\")\n",
    "# print(f\"Number of parameters: {params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got llama model, loading in half precision\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4c991a0f7c4b2bb7eb553d6c195b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('5.18e+10', 6476005376.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flops_and_params(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
