{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#set visible cuda devices\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoModelForTokenClassification, \n",
    "                          AutoModelForCausalLM,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          AutoModel,\n",
    ")\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from data_utils.model_utils import count_trainable_parameters, freeze_model, unfreeze_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn above into a function that accepts multiple models and returns the gpu memory needed\n",
    "# it should take in a list of model names and return a dictionary of model names and gpu memory needed\n",
    "\n",
    "def get_gpu_memory_needed(model_names):\n",
    "    device = torch.device('cuda:0') \n",
    "    gpu_memory_needed = {}\n",
    "    for model_name in tqdm(model_names):\n",
    "        model = AutoModel.from_pretrained(model_name,\n",
    "                                  )\n",
    "        model.to(device)\n",
    "        gpu_memory_needed[model_name] = torch.cuda.memory_allocated(device.index)/1024**3\n",
    "    return gpu_memory_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:01<00:09,  1.96s/it]Some weights of BertModel were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 2/6 [00:02<00:03,  1.09it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 3/6 [00:02<00:01,  1.53it/s]Some weights of BertModel were not initialized from the model checkpoint at nlpie/distil-biobert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  2.08it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacf96bcb76f42ae8c913cad4157ff62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"nlpie/bio-mobilebert\",\n",
    "                    \"nlpie/tiny-biobert\",\n",
    "                    \"roberta-base\",\n",
    "                    \"nlpie/distil-biobert\",\n",
    "                    \"dmis-lab/biobert-v1.1\",\n",
    "                     \"meta-llama/Llama-2-7b-hf\"\n",
    "                     ]\n",
    "\n",
    "gpu_memory_needed = get_gpu_memory_needed(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlpie/bio-mobilebert': 0.09157848358154297,\n",
       " 'nlpie/tiny-biobert': 0.05176830291748047,\n",
       " 'roberta-base': 0.46604251861572266,\n",
       " 'nlpie/distil-biobert': 0.24604511260986328,\n",
       " 'dmis-lab/biobert-v1.1': 0.40447139739990234,\n",
       " 'meta-llama/Llama-2-7b-hf': 24.739288330078125}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_memory_needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "with open('../gpu_memory_needed.json', 'w') as fp:\n",
    "    json.dump(gpu_memory_needed, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
